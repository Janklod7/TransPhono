{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PhonoTransformers.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "cN3Vko237nv5",
        "hJRtuEwh7r7S"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title Parameters\n",
        "##@markdown -  actual = images*batch_size\n",
        "\n",
        "\n",
        "use_data = True  #@param {type:\"boolean\"}\n",
        "feature_embedding = False  #@param {type:\"boolean\"}\n",
        "embed_size =  18#@param {type:\"number\"}\n",
        "device = \"cuda:0\"  #@param [\"cpu\", \"cuda:0\"]\n",
        "dropout = 0.2  #@param {type:\"number\"}\n",
        "d_hid = 200  #@param {type:\"number\"} # dimension of the feedforward network model in nn.TransformerEncoder\n",
        "nlayers =   3#@param {type:\"number\"} # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "nhead =   2#@param {type:\"number\"} # number of heads in nn.MultiheadAttention\n",
        "batch_size =  16#@param {type:\"number\"}\n",
        "eval_batch_size =  16#@param {type:\"number\"}\n",
        "epochs = 25 #@param {type:\"number\"}\n",
        "learning_rate =   0.002#@param {type:\"number\"} # learning rate\n",
        "log_interval = 100 #@param {type:\"number\"}\n",
        "maximum_word_length =  10#@param {type:\"number\"}\n",
        "\n",
        "lr = learning_rate\n",
        "emsize = embed_size\n",
        "word_names = []\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "d343FiGwL4lq",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# data and imports"
      ],
      "metadata": {
        "id": "cN3Vko237nv5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_NX5c0TLKOu"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from typing import Tuple\n",
        "\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "from torch.utils.data import dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "response = requests.get(\"http://svn.code.sf.net/p/cmusphinx/code/trunk/cmudict/cmudict-0.7b\")\n",
        "data = response.text.split(\"\\n\")\n",
        "phonemes = {}\n",
        "index_to_phon = {}\n",
        "words = {}\n",
        "maxlen = 0\n",
        "ip = 1\n",
        "toolong = 0\n",
        "toplen = maximum_word_length\n",
        "for d in data:\n",
        "  if len(d) > 0 and d[0] != \";\":\n",
        "    d = d.split(\" \")\n",
        "    phonms = [\"BEG\"] + d[2:] + [\"END\"]\n",
        "    if len(phonms) > toplen:\n",
        "      toolong += 1\n",
        "      continue\n",
        "    words[d[0]] = phonms\n",
        "    if len(phonms) > maxlen:\n",
        "      maxlen = len(phonms)\n",
        "    for p in phonms:\n",
        "      if p not in phonemes:\n",
        "        phonemes[p] = ip\n",
        "        index_to_phon[ip] = p\n",
        "        ip += 1\n",
        "\n",
        "print(f\"Got {len(words)} words\")\n",
        "print(f\"Got {len(phonemes)} phonemes\")\n",
        "print(f\"Word maximum length is {maxlen}\")\n",
        "print(f\"{toolong} Words were too long\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gh9Vay4rSDOK",
        "outputId": "77c2bfeb-9b3c-484b-83cb-e1c80195bcd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got 113601 words\n",
            "Got 71 phonemes\n",
            "Word maximum length is 10\n",
            "20251 Words were too long\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def word2vec(features):\n",
        "  return torch.tensor([phonemes[a] for a in features])\n",
        "\n",
        "# print(list(words.values())[0])\n",
        "# print(word2vec(list(words.values())[0]))\n",
        "vecs = torch.nn.utils.rnn.pad_sequence([word2vec(w) for w in words.values()], batch_first=True)\n",
        "word_names ,data = list(words.keys()), vecs\n",
        "print(data.shape)\n",
        "print(f\"all words padded to length {len(vecs[0])} where max length is {maxlen}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlJsYVOiWl50",
        "outputId": "e371458e-5416-4474-cd30-51ec71b1dc3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([113601, 10])\n",
            "all words padded to length 10 where max length is 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if use_data:\n",
        "  data_size = len(words)\n",
        "  num_of_features = emsize\n",
        "  num_of_phonemes = len(phonemes) + 1\n",
        "  word_maxlength = maxlen\n",
        "  \n",
        "else:\n",
        "  word_maxlength = 10\n",
        "  data_size = 100000\n",
        "  num_of_features = 12\n",
        "  num_of_phonemes = 30\n",
        "  emsize = num_of_features\n",
        "\n",
        "ntokens = num_of_phonemes  # size of vocabulary"
      ],
      "metadata": {
        "id": "eqBDiA99ci9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.datasets import WikiText2\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "if not use_data:\n",
        "  # pretrained_weights = torch.randint(0,num_of_features,(num_of_phonemes+1, num_of_features))\n",
        "  pretrained_weights = torch.tensor(np.random.choice([-1,0,1], (num_of_phonemes+1, num_of_features)))\n",
        "  pretrained_weights[0] = torch.zeros(num_of_features)\n",
        "  print(pretrained_weights[3])\n",
        "  generated_data = [torch.zeros(word_maxlength)]\n",
        "  for i in range(data_size):\n",
        "    length = torch.randint(2,word_maxlength,(1,))\n",
        "    generated_data.append(torch.randint(1,num_of_phonemes,(length,)))\n",
        "\n",
        "  generated_data = torch.nn.utils.rnn.pad_sequence(generated_data, batch_first=True)[1:]\n",
        "  data = generated_data\n",
        "\n",
        "train_data = data[:int(data_size*0.7)]\n",
        "train_words = word_names[:int(data_size*0.7)]\n",
        "val_data = data[int(data_size*0.7): int(data_size*0.9)]\n",
        "val_words = word_names[int(data_size*0.7): int(data_size*0.9)]\n",
        "test_data = data[int(data_size*0.9):]\n",
        "test_words = word_names[int(data_size*0.9):]\n",
        "\n",
        "print(train_data.shape)\n",
        "print(val_data.shape)\n",
        "print(test_data.shape)\n",
        "\n",
        "\n",
        "def batchify(dta: Tensor, bsz: int) -> Tensor:\n",
        "    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n",
        "    that wouldn't cleanly fit.\n",
        "\n",
        "    Args:\n",
        "        dta: Tensor, shape [N]\n",
        "        bsz: int, batch size\n",
        "\n",
        "    Returns:\n",
        "        Tensor of shape [N // bsz, bsz]\n",
        "    \"\"\"\n",
        "    seq_len = dta.size(0) // bsz\n",
        "    dta = dta[:seq_len * bsz]\n",
        "    dta = dta.view(seq_len, bsz, word_maxlength)\n",
        "    return dta.to(device)\n",
        "\n",
        "\n",
        "\n",
        "train_data = batchify(train_data, batch_size).long()  # shape [seq_len, batch_size]\n",
        "print(train_data.shape)\n",
        "val_data = batchify(val_data, eval_batch_size).long()\n",
        "print(val_data.shape)\n",
        "test_data = batchify(test_data, eval_batch_size).long()"
      ],
      "metadata": {
        "id": "E8BIbhVpNHG2",
        "outputId": "c7951d27-349a-49b9-e4a5-168555d5c5da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([79520, 10])\n",
            "torch.Size([22720, 10])\n",
            "torch.Size([11361, 10])\n",
            "torch.Size([4970, 16, 10])\n",
            "torch.Size([1420, 16, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(dataset, i):\n",
        "  return torch.tensor(dataset[i]),torch.nn.functional.one_hot(torch.tensor(dataset[i]), num_classes=num_of_phonemes)"
      ],
      "metadata": {
        "id": "sZKlVVg47-xA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_batch(train_data,0)[0].shape, get_batch(train_data,0)[1].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3I_4TXLGf9tE",
        "outputId": "4a72efcf-ae94-4353-c298-650821eb3b5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 10]) torch.Size([16, 10, 72])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "hJRtuEwh7r7S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        print(position.shape, div_term.shape, pe[:, 0, 1::2].shape)\n",
        "        # print(position * div_term, (position * div_term).shape)\n",
        "        print(torch.sin(position * div_term).shape, torch.cos(position * div_term).shape)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
        "        \"\"\"\n",
        "        # print(x.shape, self.pe[:x.size(0)].shape)\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "soID3A5NMooJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerModel(nn.Module):\n",
        "\n",
        "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
        "                 nlayers: int, dropout: float = 0.5, features = None):\n",
        "        super().__init__()\n",
        "        self.model_type = 'Transformer'\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout, max_len=max(batch_size, eval_batch_size))\n",
        "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.d_model = d_model\n",
        "        self.decoder = nn.Linear(d_model, ntoken)\n",
        "        if features is not None:\n",
        "          print(\"lulululu\")\n",
        "          self.encoder = nn.Embedding.from_pretrained(features, freeze=True)\n",
        "        else:\n",
        "          self.encoder = nn.Embedding(ntoken, d_model)\n",
        "          self.init_weights()\n",
        "        self.sftmx = nn.Softmax(dim=2)\n",
        "        \n",
        "\n",
        "        \n",
        "\n",
        "    def init_weights(self) -> None:\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src: Tensor, shape [seq_len, batch_size]\n",
        "            src_mask: Tensor, shape [seq_len, seq_len]\n",
        "\n",
        "        Returns:\n",
        "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
        "        \"\"\"\n",
        "        # print(\"lalala\",src.shape, src_mask.shape)\n",
        "        # print(\"bla\", self.encoder(src))\n",
        "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
        "        \n",
        "        src = self.pos_encoder(src)\n",
        "        # print(src.shape, src)\n",
        "        output = self.transformer_encoder(src, src_mask)\n",
        "        # print(output.shape)\n",
        "        output = self.decoder(output)\n",
        "\n",
        "        # return self.sftmx(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
        "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
        "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
      ],
      "metadata": {
        "id": "kiiuFevTL65d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "73Wh5VpZ7wZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import time\n",
        "\n",
        "if not feature_embedding:\n",
        "  pretrained_weights = None\n",
        "else:\n",
        "  pretrained_weights = pretrained_weights.float()\n",
        "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout, features=pretrained_weights).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# criterion2 = nn.L1Loss()\n",
        "\n",
        "example = train_data[0].unsqueeze(0)\n",
        "mask = torch.tensor([1]).unsqueeze(0).float().to(device)\n",
        "\n",
        "print(f\"size of embedding:\", model.encoder(example).shape)\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "sft = nn.Softmax(dim=2)\n",
        "\n",
        "def train(model: nn.Module) -> None:\n",
        "    model.train()  # turn on train mode\n",
        "    total_loss = 0.\n",
        "    \n",
        "    start_time = time.time()\n",
        "    src_mask = generate_square_subsequent_mask(batch_size).to(device)\n",
        "\n",
        "    num_batches = len(train_data) // batch_size\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        if len(src_mask) != batch_size:  # only on last batch\n",
        "            src_mask = src_mask[:batch_size, :batch_size]\n",
        "        output = model(data, src_mask)\n",
        "        # output = sft(model(data, src_mask))\n",
        "        # print(targets.float())\n",
        "        loss = 10 * criterion(output, targets.float())\n",
        "        # loss = criterion(output, targets.float()) + criterion2(output, targets.float())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            lr = scheduler.get_last_lr()[0]\n",
        "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
        "            cur_loss = total_loss / log_interval\n",
        "            ppl = math.exp(cur_loss)\n",
        "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
        "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
        "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
        "    model.eval()  # turn on evaluation mode\n",
        "    total_loss = 0.\n",
        "    src_mask = generate_square_subsequent_mask(eval_batch_size).to(device)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, eval_data.size(0) - 1, eval_batch_size):\n",
        "            data, targets = get_batch(eval_data, i)\n",
        "            \n",
        "            src_mask = src_mask[:eval_batch_size, :eval_batch_size]\n",
        "            output = model(data, src_mask)\n",
        "            # output = sft(model(data, src_mask))\n",
        "            output_flat = output\n",
        "            total_loss += eval_batch_size * criterion(output_flat, targets.float()).item()\n",
        "    return total_loss / (len(eval_data) - 1)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9OrXaeAVQ4KJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ef14c89-4e5f-431d-c7f7-bf6ab2ff67e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 1]) torch.Size([9]) torch.Size([16, 9])\n",
            "torch.Size([16, 9]) torch.Size([16, 9])\n",
            "size of embedding: torch.Size([1, 16, 10, 18])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_loss = float('inf')\n",
        "\n",
        "best_model = None\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(model)\n",
        "    val_loss = 10 * evaluate(model, val_data)\n",
        "    val_ppl = math.exp(val_loss)\n",
        "    elapsed = time.time() - epoch_start_time\n",
        "    print('-' * 89)\n",
        "    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
        "          f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
        "    print('-' * 89)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model = copy.deepcopy(model)\n",
        "\n",
        "    scheduler.step()"
      ],
      "metadata": {
        "id": "Y9S0BozzQ-_H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "18899db1-0220-4c41-e2cb-0028f8ec6e6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   100/  310 batches | lr 0.00 | ms/batch  8.94 | loss  3.23 | ppl    25.31\n",
            "| epoch   1 |   200/  310 batches | lr 0.00 | ms/batch  8.45 | loss  3.16 | ppl    23.61\n",
            "| epoch   1 |   300/  310 batches | lr 0.00 | ms/batch  8.58 | loss  3.10 | ppl    22.29\n",
            "| epoch   1 |   400/  310 batches | lr 0.00 | ms/batch  8.85 | loss  3.02 | ppl    20.47\n",
            "| epoch   1 |   500/  310 batches | lr 0.00 | ms/batch  8.73 | loss  2.94 | ppl    18.97\n",
            "| epoch   1 |   600/  310 batches | lr 0.00 | ms/batch  8.53 | loss  2.87 | ppl    17.60\n",
            "| epoch   1 |   700/  310 batches | lr 0.00 | ms/batch  8.50 | loss  2.81 | ppl    16.59\n",
            "| epoch   1 |   800/  310 batches | lr 0.00 | ms/batch  8.47 | loss  2.74 | ppl    15.51\n",
            "| epoch   1 |   900/  310 batches | lr 0.00 | ms/batch  8.55 | loss  2.70 | ppl    14.93\n",
            "| epoch   1 |  1000/  310 batches | lr 0.00 | ms/batch  8.67 | loss  2.67 | ppl    14.47\n",
            "| epoch   1 |  1100/  310 batches | lr 0.00 | ms/batch  8.60 | loss  2.66 | ppl    14.34\n",
            "| epoch   1 |  1200/  310 batches | lr 0.00 | ms/batch  8.86 | loss  2.64 | ppl    14.07\n",
            "| epoch   1 |  1300/  310 batches | lr 0.00 | ms/batch  8.79 | loss  2.61 | ppl    13.60\n",
            "| epoch   1 |  1400/  310 batches | lr 0.00 | ms/batch  8.58 | loss  2.54 | ppl    12.73\n",
            "| epoch   1 |  1500/  310 batches | lr 0.00 | ms/batch  9.18 | loss  2.50 | ppl    12.19\n",
            "| epoch   1 |  1600/  310 batches | lr 0.00 | ms/batch  9.18 | loss  2.49 | ppl    12.04\n",
            "| epoch   1 |  1700/  310 batches | lr 0.00 | ms/batch  8.43 | loss  2.47 | ppl    11.81\n",
            "| epoch   1 |  1800/  310 batches | lr 0.00 | ms/batch  8.72 | loss  2.41 | ppl    11.12\n",
            "| epoch   1 |  1900/  310 batches | lr 0.00 | ms/batch  8.67 | loss  2.40 | ppl    10.97\n",
            "| epoch   1 |  2000/  310 batches | lr 0.00 | ms/batch  8.46 | loss  2.43 | ppl    11.38\n",
            "| epoch   1 |  2100/  310 batches | lr 0.00 | ms/batch  8.57 | loss  2.42 | ppl    11.27\n",
            "| epoch   1 |  2200/  310 batches | lr 0.00 | ms/batch  8.42 | loss  2.38 | ppl    10.86\n",
            "| epoch   1 |  2300/  310 batches | lr 0.00 | ms/batch  8.43 | loss  2.37 | ppl    10.73\n",
            "| epoch   1 |  2400/  310 batches | lr 0.00 | ms/batch  8.51 | loss  2.32 | ppl    10.22\n",
            "| epoch   1 |  2500/  310 batches | lr 0.00 | ms/batch  8.57 | loss  2.31 | ppl    10.09\n",
            "| epoch   1 |  2600/  310 batches | lr 0.00 | ms/batch  8.58 | loss  2.28 | ppl     9.80\n",
            "| epoch   1 |  2700/  310 batches | lr 0.00 | ms/batch  8.30 | loss  2.25 | ppl     9.49\n",
            "| epoch   1 |  2800/  310 batches | lr 0.00 | ms/batch  8.67 | loss  2.24 | ppl     9.40\n",
            "| epoch   1 |  2900/  310 batches | lr 0.00 | ms/batch  8.45 | loss  2.20 | ppl     9.02\n",
            "| epoch   1 |  3000/  310 batches | lr 0.00 | ms/batch  8.54 | loss  2.21 | ppl     9.09\n",
            "| epoch   1 |  3100/  310 batches | lr 0.00 | ms/batch  8.22 | loss  2.20 | ppl     9.00\n",
            "| epoch   1 |  3200/  310 batches | lr 0.00 | ms/batch  8.42 | loss  2.18 | ppl     8.84\n",
            "| epoch   1 |  3300/  310 batches | lr 0.00 | ms/batch  8.59 | loss  2.10 | ppl     8.15\n",
            "| epoch   1 |  3400/  310 batches | lr 0.00 | ms/batch  8.58 | loss  2.01 | ppl     7.45\n",
            "| epoch   1 |  3500/  310 batches | lr 0.00 | ms/batch  8.32 | loss  1.99 | ppl     7.31\n",
            "| epoch   1 |  3600/  310 batches | lr 0.00 | ms/batch  9.15 | loss  2.02 | ppl     7.52\n",
            "| epoch   1 |  3700/  310 batches | lr 0.00 | ms/batch  8.40 | loss  1.98 | ppl     7.28\n",
            "| epoch   1 |  3800/  310 batches | lr 0.00 | ms/batch  8.78 | loss  1.97 | ppl     7.17\n",
            "| epoch   1 |  3900/  310 batches | lr 0.00 | ms/batch  8.52 | loss  1.96 | ppl     7.11\n",
            "| epoch   1 |  4000/  310 batches | lr 0.00 | ms/batch  8.54 | loss  1.97 | ppl     7.16\n",
            "| epoch   1 |  4100/  310 batches | lr 0.00 | ms/batch  8.42 | loss  1.91 | ppl     6.76\n",
            "| epoch   1 |  4200/  310 batches | lr 0.00 | ms/batch  8.31 | loss  1.91 | ppl     6.76\n",
            "| epoch   1 |  4300/  310 batches | lr 0.00 | ms/batch  8.36 | loss  1.90 | ppl     6.71\n",
            "| epoch   1 |  4400/  310 batches | lr 0.00 | ms/batch  8.34 | loss  1.84 | ppl     6.29\n",
            "| epoch   1 |  4500/  310 batches | lr 0.00 | ms/batch  8.79 | loss  1.80 | ppl     6.08\n",
            "| epoch   1 |  4600/  310 batches | lr 0.00 | ms/batch  8.40 | loss  1.85 | ppl     6.33\n",
            "| epoch   1 |  4700/  310 batches | lr 0.00 | ms/batch  8.53 | loss  1.89 | ppl     6.62\n",
            "| epoch   1 |  4800/  310 batches | lr 0.00 | ms/batch  8.34 | loss  1.81 | ppl     6.13\n",
            "| epoch   1 |  4900/  310 batches | lr 0.00 | ms/batch  8.60 | loss  1.78 | ppl     5.94\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 42.83s | valid loss  1.60 | valid ppl     4.97\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   100/  310 batches | lr 0.00 | ms/batch  8.91 | loss  1.79 | ppl     6.01\n",
            "| epoch   2 |   200/  310 batches | lr 0.00 | ms/batch  8.60 | loss  1.72 | ppl     5.56\n",
            "| epoch   2 |   300/  310 batches | lr 0.00 | ms/batch  8.50 | loss  1.70 | ppl     5.47\n",
            "| epoch   2 |   400/  310 batches | lr 0.00 | ms/batch  8.61 | loss  1.72 | ppl     5.60\n",
            "| epoch   2 |   500/  310 batches | lr 0.00 | ms/batch  8.53 | loss  1.63 | ppl     5.12\n",
            "| epoch   2 |   600/  310 batches | lr 0.00 | ms/batch  9.17 | loss  1.62 | ppl     5.04\n",
            "| epoch   2 |   700/  310 batches | lr 0.00 | ms/batch  8.80 | loss  1.60 | ppl     4.93\n",
            "| epoch   2 |   800/  310 batches | lr 0.00 | ms/batch  8.53 | loss  1.57 | ppl     4.81\n",
            "| epoch   2 |   900/  310 batches | lr 0.00 | ms/batch  8.58 | loss  1.58 | ppl     4.84\n",
            "| epoch   2 |  1000/  310 batches | lr 0.00 | ms/batch  8.61 | loss  1.55 | ppl     4.71\n",
            "| epoch   2 |  1100/  310 batches | lr 0.00 | ms/batch  8.57 | loss  1.56 | ppl     4.77\n",
            "| epoch   2 |  1200/  310 batches | lr 0.00 | ms/batch  8.71 | loss  1.61 | ppl     5.01\n",
            "| epoch   2 |  1300/  310 batches | lr 0.00 | ms/batch  8.72 | loss  1.49 | ppl     4.43\n",
            "| epoch   2 |  1400/  310 batches | lr 0.00 | ms/batch  8.56 | loss  1.46 | ppl     4.29\n",
            "| epoch   2 |  1500/  310 batches | lr 0.00 | ms/batch  8.47 | loss  1.48 | ppl     4.41\n",
            "| epoch   2 |  1600/  310 batches | lr 0.00 | ms/batch  8.61 | loss  1.49 | ppl     4.43\n",
            "| epoch   2 |  1700/  310 batches | lr 0.00 | ms/batch  8.45 | loss  1.52 | ppl     4.57\n",
            "| epoch   2 |  1800/  310 batches | lr 0.00 | ms/batch  8.57 | loss  1.46 | ppl     4.29\n",
            "| epoch   2 |  1900/  310 batches | lr 0.00 | ms/batch  8.60 | loss  1.50 | ppl     4.48\n",
            "| epoch   2 |  2000/  310 batches | lr 0.00 | ms/batch  8.79 | loss  1.44 | ppl     4.23\n",
            "| epoch   2 |  2100/  310 batches | lr 0.00 | ms/batch  8.59 | loss  1.52 | ppl     4.56\n",
            "| epoch   2 |  2200/  310 batches | lr 0.00 | ms/batch  8.59 | loss  1.46 | ppl     4.32\n",
            "| epoch   2 |  2300/  310 batches | lr 0.00 | ms/batch  8.54 | loss  1.45 | ppl     4.28\n",
            "| epoch   2 |  2400/  310 batches | lr 0.00 | ms/batch  8.68 | loss  1.42 | ppl     4.14\n",
            "| epoch   2 |  2500/  310 batches | lr 0.00 | ms/batch  8.72 | loss  1.42 | ppl     4.13\n",
            "| epoch   2 |  2600/  310 batches | lr 0.00 | ms/batch  8.68 | loss  1.38 | ppl     3.97\n",
            "| epoch   2 |  2700/  310 batches | lr 0.00 | ms/batch  8.76 | loss  1.37 | ppl     3.93\n",
            "| epoch   2 |  2800/  310 batches | lr 0.00 | ms/batch  8.79 | loss  1.39 | ppl     4.01\n",
            "| epoch   2 |  2900/  310 batches | lr 0.00 | ms/batch  8.48 | loss  1.39 | ppl     4.01\n",
            "| epoch   2 |  3000/  310 batches | lr 0.00 | ms/batch  8.63 | loss  1.41 | ppl     4.09\n",
            "| epoch   2 |  3100/  310 batches | lr 0.00 | ms/batch  8.80 | loss  1.43 | ppl     4.18\n",
            "| epoch   2 |  3200/  310 batches | lr 0.00 | ms/batch  8.54 | loss  1.40 | ppl     4.05\n",
            "| epoch   2 |  3300/  310 batches | lr 0.00 | ms/batch  8.44 | loss  1.38 | ppl     3.97\n",
            "| epoch   2 |  3400/  310 batches | lr 0.00 | ms/batch  8.61 | loss  1.27 | ppl     3.55\n",
            "| epoch   2 |  3500/  310 batches | lr 0.00 | ms/batch  8.68 | loss  1.27 | ppl     3.55\n",
            "| epoch   2 |  3600/  310 batches | lr 0.00 | ms/batch  8.77 | loss  1.24 | ppl     3.46\n",
            "| epoch   2 |  3700/  310 batches | lr 0.00 | ms/batch  8.85 | loss  1.22 | ppl     3.39\n",
            "| epoch   2 |  3800/  310 batches | lr 0.00 | ms/batch  8.63 | loss  1.23 | ppl     3.41\n",
            "| epoch   2 |  3900/  310 batches | lr 0.00 | ms/batch  8.70 | loss  1.23 | ppl     3.44\n",
            "| epoch   2 |  4000/  310 batches | lr 0.00 | ms/batch  8.71 | loss  1.21 | ppl     3.37\n",
            "| epoch   2 |  4100/  310 batches | lr 0.00 | ms/batch  8.74 | loss  1.12 | ppl     3.07\n",
            "| epoch   2 |  4200/  310 batches | lr 0.00 | ms/batch  8.88 | loss  1.20 | ppl     3.31\n",
            "| epoch   2 |  4300/  310 batches | lr 0.00 | ms/batch  8.72 | loss  1.23 | ppl     3.42\n",
            "| epoch   2 |  4400/  310 batches | lr 0.00 | ms/batch  8.97 | loss  1.20 | ppl     3.32\n",
            "| epoch   2 |  4500/  310 batches | lr 0.00 | ms/batch  8.72 | loss  1.19 | ppl     3.30\n",
            "| epoch   2 |  4600/  310 batches | lr 0.00 | ms/batch  8.63 | loss  1.23 | ppl     3.43\n",
            "| epoch   2 |  4700/  310 batches | lr 0.00 | ms/batch  8.58 | loss  1.27 | ppl     3.56\n",
            "| epoch   2 |  4800/  310 batches | lr 0.00 | ms/batch  8.44 | loss  1.15 | ppl     3.16\n",
            "| epoch   2 |  4900/  310 batches | lr 0.00 | ms/batch  8.62 | loss  1.13 | ppl     3.11\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 43.23s | valid loss  0.99 | valid ppl     2.70\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   100/  310 batches | lr 0.00 | ms/batch  8.82 | loss  1.18 | ppl     3.25\n",
            "| epoch   3 |   200/  310 batches | lr 0.00 | ms/batch  9.76 | loss  1.10 | ppl     3.02\n",
            "| epoch   3 |   300/  310 batches | lr 0.00 | ms/batch  8.87 | loss  1.09 | ppl     2.98\n",
            "| epoch   3 |   400/  310 batches | lr 0.00 | ms/batch  9.02 | loss  1.16 | ppl     3.19\n",
            "| epoch   3 |   500/  310 batches | lr 0.00 | ms/batch  8.78 | loss  1.06 | ppl     2.89\n",
            "| epoch   3 |   600/  310 batches | lr 0.00 | ms/batch  8.45 | loss  1.07 | ppl     2.90\n",
            "| epoch   3 |   700/  310 batches | lr 0.00 | ms/batch  8.97 | loss  1.08 | ppl     2.93\n",
            "| epoch   3 |   800/  310 batches | lr 0.00 | ms/batch  8.68 | loss  1.07 | ppl     2.91\n",
            "| epoch   3 |   900/  310 batches | lr 0.00 | ms/batch  8.71 | loss  1.08 | ppl     2.93\n",
            "| epoch   3 |  1000/  310 batches | lr 0.00 | ms/batch  8.58 | loss  1.03 | ppl     2.81\n",
            "| epoch   3 |  1100/  310 batches | lr 0.00 | ms/batch  8.85 | loss  1.04 | ppl     2.82\n",
            "| epoch   3 |  1200/  310 batches | lr 0.00 | ms/batch  8.54 | loss  1.12 | ppl     3.07\n",
            "| epoch   3 |  1300/  310 batches | lr 0.00 | ms/batch  8.53 | loss  0.97 | ppl     2.64\n",
            "| epoch   3 |  1400/  310 batches | lr 0.00 | ms/batch  8.69 | loss  0.97 | ppl     2.63\n",
            "| epoch   3 |  1500/  310 batches | lr 0.00 | ms/batch  8.88 | loss  1.02 | ppl     2.77\n",
            "| epoch   3 |  1600/  310 batches | lr 0.00 | ms/batch  8.69 | loss  1.00 | ppl     2.72\n",
            "| epoch   3 |  1700/  310 batches | lr 0.00 | ms/batch  9.30 | loss  1.03 | ppl     2.79\n",
            "| epoch   3 |  1800/  310 batches | lr 0.00 | ms/batch  8.92 | loss  1.00 | ppl     2.71\n",
            "| epoch   3 |  1900/  310 batches | lr 0.00 | ms/batch  8.80 | loss  1.08 | ppl     2.95\n",
            "| epoch   3 |  2000/  310 batches | lr 0.00 | ms/batch  8.73 | loss  0.97 | ppl     2.65\n",
            "| epoch   3 |  2100/  310 batches | lr 0.00 | ms/batch  8.79 | loss  1.05 | ppl     2.86\n",
            "| epoch   3 |  2200/  310 batches | lr 0.00 | ms/batch  9.11 | loss  0.99 | ppl     2.70\n",
            "| epoch   3 |  2300/  310 batches | lr 0.00 | ms/batch  9.19 | loss  1.00 | ppl     2.71\n",
            "| epoch   3 |  2400/  310 batches | lr 0.00 | ms/batch  9.18 | loss  0.97 | ppl     2.63\n",
            "| epoch   3 |  2500/  310 batches | lr 0.00 | ms/batch  9.65 | loss  0.97 | ppl     2.63\n",
            "| epoch   3 |  2600/  310 batches | lr 0.00 | ms/batch  8.96 | loss  0.93 | ppl     2.54\n",
            "| epoch   3 |  2700/  310 batches | lr 0.00 | ms/batch  8.87 | loss  0.94 | ppl     2.56\n",
            "| epoch   3 |  2800/  310 batches | lr 0.00 | ms/batch  9.07 | loss  0.95 | ppl     2.59\n",
            "| epoch   3 |  2900/  310 batches | lr 0.00 | ms/batch  9.57 | loss  0.97 | ppl     2.63\n",
            "| epoch   3 |  3000/  310 batches | lr 0.00 | ms/batch  8.76 | loss  0.99 | ppl     2.69\n",
            "| epoch   3 |  3100/  310 batches | lr 0.00 | ms/batch  8.69 | loss  1.03 | ppl     2.80\n",
            "| epoch   3 |  3200/  310 batches | lr 0.00 | ms/batch  8.85 | loss  1.00 | ppl     2.72\n",
            "| epoch   3 |  3300/  310 batches | lr 0.00 | ms/batch  8.60 | loss  1.01 | ppl     2.75\n",
            "| epoch   3 |  3400/  310 batches | lr 0.00 | ms/batch  9.07 | loss  0.92 | ppl     2.52\n",
            "| epoch   3 |  3500/  310 batches | lr 0.00 | ms/batch  8.65 | loss  0.94 | ppl     2.55\n",
            "| epoch   3 |  3600/  310 batches | lr 0.00 | ms/batch  9.50 | loss  0.89 | ppl     2.44\n",
            "| epoch   3 |  3700/  310 batches | lr 0.00 | ms/batch  8.76 | loss  0.90 | ppl     2.45\n",
            "| epoch   3 |  3800/  310 batches | lr 0.00 | ms/batch  8.63 | loss  0.91 | ppl     2.48\n",
            "| epoch   3 |  3900/  310 batches | lr 0.00 | ms/batch  8.87 | loss  0.88 | ppl     2.41\n",
            "| epoch   3 |  4000/  310 batches | lr 0.00 | ms/batch  8.80 | loss  0.82 | ppl     2.27\n",
            "| epoch   3 |  4100/  310 batches | lr 0.00 | ms/batch  9.14 | loss  0.74 | ppl     2.09\n",
            "| epoch   3 |  4200/  310 batches | lr 0.00 | ms/batch  9.00 | loss  0.83 | ppl     2.30\n",
            "| epoch   3 |  4300/  310 batches | lr 0.00 | ms/batch  8.80 | loss  0.88 | ppl     2.40\n",
            "| epoch   3 |  4400/  310 batches | lr 0.00 | ms/batch  8.72 | loss  0.89 | ppl     2.44\n",
            "| epoch   3 |  4500/  310 batches | lr 0.00 | ms/batch  8.88 | loss  0.91 | ppl     2.48\n",
            "| epoch   3 |  4600/  310 batches | lr 0.00 | ms/batch  8.82 | loss  0.92 | ppl     2.52\n",
            "| epoch   3 |  4700/  310 batches | lr 0.00 | ms/batch  8.83 | loss  0.94 | ppl     2.55\n",
            "| epoch   3 |  4800/  310 batches | lr 0.00 | ms/batch  9.36 | loss  0.83 | ppl     2.29\n",
            "| epoch   3 |  4900/  310 batches | lr 0.00 | ms/batch  9.15 | loss  0.81 | ppl     2.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 44.48s | valid loss  0.71 | valid ppl     2.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |   100/  310 batches | lr 0.00 | ms/batch  8.91 | loss  0.86 | ppl     2.37\n",
            "| epoch   4 |   200/  310 batches | lr 0.00 | ms/batch  8.69 | loss  0.82 | ppl     2.27\n",
            "| epoch   4 |   300/  310 batches | lr 0.00 | ms/batch  8.71 | loss  0.81 | ppl     2.24\n",
            "| epoch   4 |   400/  310 batches | lr 0.00 | ms/batch  8.88 | loss  0.88 | ppl     2.42\n",
            "| epoch   4 |   500/  310 batches | lr 0.00 | ms/batch  8.72 | loss  0.79 | ppl     2.20\n",
            "| epoch   4 |   600/  310 batches | lr 0.00 | ms/batch  9.24 | loss  0.80 | ppl     2.22\n",
            "| epoch   4 |   700/  310 batches | lr 0.00 | ms/batch  9.02 | loss  0.81 | ppl     2.25\n",
            "| epoch   4 |   800/  310 batches | lr 0.00 | ms/batch  8.76 | loss  0.81 | ppl     2.24\n",
            "| epoch   4 |   900/  310 batches | lr 0.00 | ms/batch  9.05 | loss  0.81 | ppl     2.24\n",
            "| epoch   4 |  1000/  310 batches | lr 0.00 | ms/batch  8.79 | loss  0.78 | ppl     2.17\n",
            "| epoch   4 |  1100/  310 batches | lr 0.00 | ms/batch  8.78 | loss  0.77 | ppl     2.17\n",
            "| epoch   4 |  1200/  310 batches | lr 0.00 | ms/batch  8.74 | loss  0.86 | ppl     2.37\n",
            "| epoch   4 |  1300/  310 batches | lr 0.00 | ms/batch  8.80 | loss  0.71 | ppl     2.04\n",
            "| epoch   4 |  1400/  310 batches | lr 0.00 | ms/batch  8.86 | loss  0.72 | ppl     2.06\n",
            "| epoch   4 |  1500/  310 batches | lr 0.00 | ms/batch  8.68 | loss  0.78 | ppl     2.18\n",
            "| epoch   4 |  1600/  310 batches | lr 0.00 | ms/batch  8.48 | loss  0.75 | ppl     2.11\n",
            "| epoch   4 |  1700/  310 batches | lr 0.00 | ms/batch  8.79 | loss  0.76 | ppl     2.14\n",
            "| epoch   4 |  1800/  310 batches | lr 0.00 | ms/batch  8.90 | loss  0.76 | ppl     2.14\n",
            "| epoch   4 |  1900/  310 batches | lr 0.00 | ms/batch  9.03 | loss  0.86 | ppl     2.36\n",
            "| epoch   4 |  2000/  310 batches | lr 0.00 | ms/batch  9.02 | loss  0.75 | ppl     2.11\n",
            "| epoch   4 |  2100/  310 batches | lr 0.00 | ms/batch  9.13 | loss  0.81 | ppl     2.25\n",
            "| epoch   4 |  2200/  310 batches | lr 0.00 | ms/batch  9.08 | loss  0.76 | ppl     2.14\n",
            "| epoch   4 |  2300/  310 batches | lr 0.00 | ms/batch  8.68 | loss  0.76 | ppl     2.13\n",
            "| epoch   4 |  2400/  310 batches | lr 0.00 | ms/batch  8.80 | loss  0.74 | ppl     2.10\n",
            "| epoch   4 |  2500/  310 batches | lr 0.00 | ms/batch  9.06 | loss  0.75 | ppl     2.12\n",
            "| epoch   4 |  2600/  310 batches | lr 0.00 | ms/batch  8.59 | loss  0.72 | ppl     2.05\n",
            "| epoch   4 |  2700/  310 batches | lr 0.00 | ms/batch  8.71 | loss  0.72 | ppl     2.06\n",
            "| epoch   4 |  2800/  310 batches | lr 0.00 | ms/batch  8.74 | loss  0.73 | ppl     2.07\n",
            "| epoch   4 |  2900/  310 batches | lr 0.00 | ms/batch  8.70 | loss  0.75 | ppl     2.11\n",
            "| epoch   4 |  3000/  310 batches | lr 0.00 | ms/batch  8.84 | loss  0.77 | ppl     2.16\n",
            "| epoch   4 |  3100/  310 batches | lr 0.00 | ms/batch  9.20 | loss  0.82 | ppl     2.26\n",
            "| epoch   4 |  3200/  310 batches | lr 0.00 | ms/batch  9.02 | loss  0.80 | ppl     2.22\n",
            "| epoch   4 |  3300/  310 batches | lr 0.00 | ms/batch  8.60 | loss  0.82 | ppl     2.28\n",
            "| epoch   4 |  3400/  310 batches | lr 0.00 | ms/batch  9.09 | loss  0.75 | ppl     2.12\n",
            "| epoch   4 |  3500/  310 batches | lr 0.00 | ms/batch  8.60 | loss  0.77 | ppl     2.16\n",
            "| epoch   4 |  3600/  310 batches | lr 0.00 | ms/batch  9.28 | loss  0.72 | ppl     2.05\n",
            "| epoch   4 |  3700/  310 batches | lr 0.00 | ms/batch  9.16 | loss  0.74 | ppl     2.09\n",
            "| epoch   4 |  3800/  310 batches | lr 0.00 | ms/batch  8.83 | loss  0.75 | ppl     2.11\n",
            "| epoch   4 |  3900/  310 batches | lr 0.00 | ms/batch  8.67 | loss  0.71 | ppl     2.04\n",
            "| epoch   4 |  4000/  310 batches | lr 0.00 | ms/batch  8.63 | loss  0.63 | ppl     1.88\n",
            "| epoch   4 |  4100/  310 batches | lr 0.00 | ms/batch  8.68 | loss  0.57 | ppl     1.77\n",
            "| epoch   4 |  4200/  310 batches | lr 0.00 | ms/batch  8.83 | loss  0.66 | ppl     1.93\n",
            "| epoch   4 |  4300/  310 batches | lr 0.00 | ms/batch  8.44 | loss  0.70 | ppl     2.02\n",
            "| epoch   4 |  4400/  310 batches | lr 0.00 | ms/batch  9.18 | loss  0.74 | ppl     2.09\n",
            "| epoch   4 |  4500/  310 batches | lr 0.00 | ms/batch  9.29 | loss  0.75 | ppl     2.12\n",
            "| epoch   4 |  4600/  310 batches | lr 0.00 | ms/batch  9.36 | loss  0.75 | ppl     2.13\n",
            "| epoch   4 |  4700/  310 batches | lr 0.00 | ms/batch  8.91 | loss  0.77 | ppl     2.15\n",
            "| epoch   4 |  4800/  310 batches | lr 0.00 | ms/batch  8.59 | loss  0.67 | ppl     1.96\n",
            "| epoch   4 |  4900/  310 batches | lr 0.00 | ms/batch  8.65 | loss  0.66 | ppl     1.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 44.25s | valid loss  0.58 | valid ppl     1.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |   100/  310 batches | lr 0.00 | ms/batch  8.96 | loss  0.71 | ppl     2.04\n",
            "| epoch   5 |   200/  310 batches | lr 0.00 | ms/batch  8.73 | loss  0.68 | ppl     1.97\n",
            "| epoch   5 |   300/  310 batches | lr 0.00 | ms/batch  8.78 | loss  0.66 | ppl     1.94\n",
            "| epoch   5 |   400/  310 batches | lr 0.00 | ms/batch  9.09 | loss  0.74 | ppl     2.10\n",
            "| epoch   5 |   500/  310 batches | lr 0.00 | ms/batch  8.82 | loss  0.64 | ppl     1.90\n",
            "| epoch   5 |   600/  310 batches | lr 0.00 | ms/batch  9.18 | loss  0.66 | ppl     1.94\n",
            "| epoch   5 |   700/  310 batches | lr 0.00 | ms/batch  8.78 | loss  0.68 | ppl     1.97\n",
            "| epoch   5 |   800/  310 batches | lr 0.00 | ms/batch  8.76 | loss  0.67 | ppl     1.96\n",
            "| epoch   5 |   900/  310 batches | lr 0.00 | ms/batch  9.01 | loss  0.65 | ppl     1.92\n",
            "| epoch   5 |  1000/  310 batches | lr 0.00 | ms/batch  8.89 | loss  0.64 | ppl     1.90\n",
            "| epoch   5 |  1100/  310 batches | lr 0.00 | ms/batch  9.13 | loss  0.63 | ppl     1.88\n",
            "| epoch   5 |  1200/  310 batches | lr 0.00 | ms/batch  8.79 | loss  0.72 | ppl     2.05\n",
            "| epoch   5 |  1300/  310 batches | lr 0.00 | ms/batch  8.94 | loss  0.58 | ppl     1.79\n",
            "| epoch   5 |  1400/  310 batches | lr 0.00 | ms/batch  8.83 | loss  0.59 | ppl     1.81\n",
            "| epoch   5 |  1500/  310 batches | lr 0.00 | ms/batch  8.68 | loss  0.65 | ppl     1.92\n",
            "| epoch   5 |  1600/  310 batches | lr 0.00 | ms/batch  8.82 | loss  0.62 | ppl     1.85\n",
            "| epoch   5 |  1700/  310 batches | lr 0.00 | ms/batch  9.04 | loss  0.62 | ppl     1.86\n",
            "| epoch   5 |  1800/  310 batches | lr 0.00 | ms/batch  9.04 | loss  0.64 | ppl     1.89\n",
            "| epoch   5 |  1900/  310 batches | lr 0.00 | ms/batch  8.90 | loss  0.74 | ppl     2.09\n",
            "| epoch   5 |  2000/  310 batches | lr 0.00 | ms/batch  9.18 | loss  0.63 | ppl     1.88\n",
            "| epoch   5 |  2100/  310 batches | lr 0.00 | ms/batch  8.96 | loss  0.68 | ppl     1.97\n",
            "| epoch   5 |  2200/  310 batches | lr 0.00 | ms/batch  8.79 | loss  0.64 | ppl     1.90\n",
            "| epoch   5 |  2300/  310 batches | lr 0.00 | ms/batch  9.00 | loss  0.63 | ppl     1.88\n",
            "| epoch   5 |  2400/  310 batches | lr 0.00 | ms/batch  8.88 | loss  0.62 | ppl     1.86\n",
            "| epoch   5 |  2500/  310 batches | lr 0.00 | ms/batch  8.80 | loss  0.64 | ppl     1.90\n",
            "| epoch   5 |  2600/  310 batches | lr 0.00 | ms/batch  8.70 | loss  0.61 | ppl     1.84\n",
            "| epoch   5 |  2700/  310 batches | lr 0.00 | ms/batch  8.80 | loss  0.61 | ppl     1.85\n",
            "| epoch   5 |  2800/  310 batches | lr 0.00 | ms/batch  9.18 | loss  0.61 | ppl     1.84\n",
            "| epoch   5 |  2900/  310 batches | lr 0.00 | ms/batch  9.04 | loss  0.64 | ppl     1.89\n",
            "| epoch   5 |  3000/  310 batches | lr 0.00 | ms/batch  9.32 | loss  0.65 | ppl     1.92\n",
            "| epoch   5 |  3100/  310 batches | lr 0.00 | ms/batch  8.86 | loss  0.70 | ppl     2.01\n",
            "| epoch   5 |  3200/  310 batches | lr 0.00 | ms/batch  9.21 | loss  0.68 | ppl     1.98\n",
            "| epoch   5 |  3300/  310 batches | lr 0.00 | ms/batch  8.79 | loss  0.72 | ppl     2.05\n",
            "| epoch   5 |  3400/  310 batches | lr 0.00 | ms/batch  8.74 | loss  0.67 | ppl     1.95\n",
            "| epoch   5 |  3500/  310 batches | lr 0.00 | ms/batch  9.01 | loss  0.68 | ppl     1.98\n",
            "| epoch   5 |  3600/  310 batches | lr 0.00 | ms/batch  8.75 | loss  0.62 | ppl     1.86\n",
            "| epoch   5 |  3700/  310 batches | lr 0.00 | ms/batch  8.80 | loss  0.65 | ppl     1.92\n",
            "| epoch   5 |  3800/  310 batches | lr 0.00 | ms/batch  8.74 | loss  0.66 | ppl     1.94\n",
            "| epoch   5 |  3900/  310 batches | lr 0.00 | ms/batch  8.70 | loss  0.62 | ppl     1.86\n",
            "| epoch   5 |  4000/  310 batches | lr 0.00 | ms/batch  8.83 | loss  0.54 | ppl     1.71\n",
            "| epoch   5 |  4100/  310 batches | lr 0.00 | ms/batch  8.73 | loss  0.49 | ppl     1.63\n",
            "| epoch   5 |  4200/  310 batches | lr 0.00 | ms/batch  8.87 | loss  0.57 | ppl     1.77\n",
            "| epoch   5 |  4300/  310 batches | lr 0.00 | ms/batch  8.95 | loss  0.61 | ppl     1.84\n",
            "| epoch   5 |  4400/  310 batches | lr 0.00 | ms/batch  8.71 | loss  0.66 | ppl     1.93\n",
            "| epoch   5 |  4500/  310 batches | lr 0.00 | ms/batch  8.60 | loss  0.66 | ppl     1.94\n",
            "| epoch   5 |  4600/  310 batches | lr 0.00 | ms/batch  8.79 | loss  0.67 | ppl     1.95\n",
            "| epoch   5 |  4700/  310 batches | lr 0.00 | ms/batch  8.82 | loss  0.66 | ppl     1.94\n",
            "| epoch   5 |  4800/  310 batches | lr 0.00 | ms/batch  8.77 | loss  0.58 | ppl     1.79\n",
            "| epoch   5 |  4900/  310 batches | lr 0.00 | ms/batch  8.83 | loss  0.58 | ppl     1.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 44.32s | valid loss  0.52 | valid ppl     1.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |   100/  310 batches | lr 0.00 | ms/batch  9.00 | loss  0.62 | ppl     1.86\n",
            "| epoch   6 |   200/  310 batches | lr 0.00 | ms/batch  8.82 | loss  0.59 | ppl     1.81\n",
            "| epoch   6 |   300/  310 batches | lr 0.00 | ms/batch  8.61 | loss  0.58 | ppl     1.79\n",
            "| epoch   6 |   400/  310 batches | lr 0.00 | ms/batch  8.94 | loss  0.67 | ppl     1.95\n",
            "| epoch   6 |   500/  310 batches | lr 0.00 | ms/batch  8.63 | loss  0.57 | ppl     1.77\n",
            "| epoch   6 |   600/  310 batches | lr 0.00 | ms/batch  8.80 | loss  0.58 | ppl     1.79\n",
            "| epoch   6 |   700/  310 batches | lr 0.00 | ms/batch  8.88 | loss  0.60 | ppl     1.83\n",
            "| epoch   6 |   800/  310 batches | lr 0.00 | ms/batch  8.98 | loss  0.59 | ppl     1.81\n",
            "| epoch   6 |   900/  310 batches | lr 0.00 | ms/batch  9.35 | loss  0.56 | ppl     1.75\n",
            "| epoch   6 |  1000/  310 batches | lr 0.00 | ms/batch  8.83 | loss  0.57 | ppl     1.76\n",
            "| epoch   6 |  1100/  310 batches | lr 0.00 | ms/batch  8.77 | loss  0.55 | ppl     1.74\n",
            "| epoch   6 |  1200/  310 batches | lr 0.00 | ms/batch  8.63 | loss  0.63 | ppl     1.87\n",
            "| epoch   6 |  1300/  310 batches | lr 0.00 | ms/batch  8.87 | loss  0.51 | ppl     1.66\n",
            "| epoch   6 |  1400/  310 batches | lr 0.00 | ms/batch  8.78 | loss  0.53 | ppl     1.69\n",
            "| epoch   6 |  1500/  310 batches | lr 0.00 | ms/batch  9.05 | loss  0.58 | ppl     1.79\n",
            "| epoch   6 |  1600/  310 batches | lr 0.00 | ms/batch  8.68 | loss  0.55 | ppl     1.73\n",
            "| epoch   6 |  1700/  310 batches | lr 0.00 | ms/batch  8.68 | loss  0.55 | ppl     1.73\n",
            "| epoch   6 |  1800/  310 batches | lr 0.00 | ms/batch  8.90 | loss  0.57 | ppl     1.77\n",
            "| epoch   6 |  1900/  310 batches | lr 0.00 | ms/batch  9.02 | loss  0.67 | ppl     1.95\n",
            "| epoch   6 |  2000/  310 batches | lr 0.00 | ms/batch  8.90 | loss  0.56 | ppl     1.76\n",
            "| epoch   6 |  2100/  310 batches | lr 0.00 | ms/batch  9.22 | loss  0.60 | ppl     1.83\n",
            "| epoch   6 |  2200/  310 batches | lr 0.00 | ms/batch  9.22 | loss  0.58 | ppl     1.78\n",
            "| epoch   6 |  2300/  310 batches | lr 0.00 | ms/batch  9.04 | loss  0.56 | ppl     1.76\n",
            "| epoch   6 |  2400/  310 batches | lr 0.00 | ms/batch  8.64 | loss  0.55 | ppl     1.74\n",
            "| epoch   6 |  2500/  310 batches | lr 0.00 | ms/batch  8.87 | loss  0.58 | ppl     1.79\n",
            "| epoch   6 |  2600/  310 batches | lr 0.00 | ms/batch  9.71 | loss  0.55 | ppl     1.74\n",
            "| epoch   6 |  2700/  310 batches | lr 0.00 | ms/batch  9.58 | loss  0.55 | ppl     1.73\n",
            "| epoch   6 |  2800/  310 batches | lr 0.00 | ms/batch  8.90 | loss  0.55 | ppl     1.73\n",
            "| epoch   6 |  2900/  310 batches | lr 0.00 | ms/batch  8.86 | loss  0.58 | ppl     1.78\n",
            "| epoch   6 |  3000/  310 batches | lr 0.00 | ms/batch  8.80 | loss  0.59 | ppl     1.80\n",
            "| epoch   6 |  3100/  310 batches | lr 0.00 | ms/batch  8.77 | loss  0.63 | ppl     1.88\n",
            "| epoch   6 |  3200/  310 batches | lr 0.00 | ms/batch  8.64 | loss  0.61 | ppl     1.84\n",
            "| epoch   6 |  3300/  310 batches | lr 0.00 | ms/batch  9.19 | loss  0.66 | ppl     1.94\n",
            "| epoch   6 |  3400/  310 batches | lr 0.00 | ms/batch  8.99 | loss  0.61 | ppl     1.85\n",
            "| epoch   6 |  3500/  310 batches | lr 0.00 | ms/batch  8.85 | loss  0.63 | ppl     1.89\n",
            "| epoch   6 |  3600/  310 batches | lr 0.00 | ms/batch  8.97 | loss  0.57 | ppl     1.77\n",
            "| epoch   6 |  3700/  310 batches | lr 0.00 | ms/batch  8.94 | loss  0.61 | ppl     1.83\n",
            "| epoch   6 |  3800/  310 batches | lr 0.00 | ms/batch  8.79 | loss  0.61 | ppl     1.85\n",
            "| epoch   6 |  3900/  310 batches | lr 0.00 | ms/batch  8.90 | loss  0.57 | ppl     1.77\n",
            "| epoch   6 |  4000/  310 batches | lr 0.00 | ms/batch  9.00 | loss  0.48 | ppl     1.62\n",
            "| epoch   6 |  4100/  310 batches | lr 0.00 | ms/batch  9.02 | loss  0.44 | ppl     1.55\n",
            "| epoch   6 |  4200/  310 batches | lr 0.00 | ms/batch  8.83 | loss  0.52 | ppl     1.68\n",
            "| epoch   6 |  4300/  310 batches | lr 0.00 | ms/batch 10.57 | loss  0.55 | ppl     1.74\n",
            "| epoch   6 |  4400/  310 batches | lr 0.00 | ms/batch  9.83 | loss  0.61 | ppl     1.84\n",
            "| epoch   6 |  4500/  310 batches | lr 0.00 | ms/batch  9.11 | loss  0.61 | ppl     1.84\n",
            "| epoch   6 |  4600/  310 batches | lr 0.00 | ms/batch  8.59 | loss  0.61 | ppl     1.84\n",
            "| epoch   6 |  4700/  310 batches | lr 0.00 | ms/batch  9.01 | loss  0.60 | ppl     1.83\n",
            "| epoch   6 |  4800/  310 batches | lr 0.00 | ms/batch  8.73 | loss  0.53 | ppl     1.70\n",
            "| epoch   6 |  4900/  310 batches | lr 0.00 | ms/batch  8.68 | loss  0.53 | ppl     1.70\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 44.73s | valid loss  0.48 | valid ppl     1.62\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |   100/  310 batches | lr 0.00 | ms/batch  9.04 | loss  0.57 | ppl     1.77\n",
            "| epoch   7 |   200/  310 batches | lr 0.00 | ms/batch  9.01 | loss  0.55 | ppl     1.73\n",
            "| epoch   7 |   300/  310 batches | lr 0.00 | ms/batch  8.96 | loss  0.53 | ppl     1.70\n",
            "| epoch   7 |   400/  310 batches | lr 0.00 | ms/batch  8.78 | loss  0.62 | ppl     1.86\n",
            "| epoch   7 |   500/  310 batches | lr 0.00 | ms/batch  8.71 | loss  0.52 | ppl     1.69\n",
            "| epoch   7 |   600/  310 batches | lr 0.00 | ms/batch  8.83 | loss  0.54 | ppl     1.71\n",
            "| epoch   7 |   700/  310 batches | lr 0.00 | ms/batch  9.22 | loss  0.56 | ppl     1.75\n",
            "| epoch   7 |   800/  310 batches | lr 0.00 | ms/batch  8.85 | loss  0.55 | ppl     1.73\n",
            "| epoch   7 |   900/  310 batches | lr 0.00 | ms/batch  8.81 | loss  0.51 | ppl     1.67\n",
            "| epoch   7 |  1000/  310 batches | lr 0.00 | ms/batch  9.03 | loss  0.52 | ppl     1.68\n",
            "| epoch   7 |  1100/  310 batches | lr 0.00 | ms/batch  8.83 | loss  0.50 | ppl     1.65\n",
            "| epoch   7 |  1200/  310 batches | lr 0.00 | ms/batch  8.73 | loss  0.58 | ppl     1.79\n",
            "| epoch   7 |  1300/  310 batches | lr 0.00 | ms/batch  8.58 | loss  0.46 | ppl     1.59\n",
            "| epoch   7 |  1400/  310 batches | lr 0.00 | ms/batch  8.63 | loss  0.48 | ppl     1.62\n",
            "| epoch   7 |  1500/  310 batches | lr 0.00 | ms/batch  8.51 | loss  0.54 | ppl     1.72\n",
            "| epoch   7 |  1600/  310 batches | lr 0.00 | ms/batch  8.84 | loss  0.50 | ppl     1.66\n",
            "| epoch   7 |  1700/  310 batches | lr 0.00 | ms/batch  8.92 | loss  0.50 | ppl     1.65\n",
            "| epoch   7 |  1800/  310 batches | lr 0.00 | ms/batch  8.79 | loss  0.54 | ppl     1.71\n",
            "| epoch   7 |  1900/  310 batches | lr 0.00 | ms/batch  8.87 | loss  0.63 | ppl     1.88\n",
            "| epoch   7 |  2000/  310 batches | lr 0.00 | ms/batch  8.78 | loss  0.52 | ppl     1.69\n",
            "| epoch   7 |  2100/  310 batches | lr 0.00 | ms/batch  8.57 | loss  0.56 | ppl     1.75\n",
            "| epoch   7 |  2200/  310 batches | lr 0.00 | ms/batch  8.71 | loss  0.54 | ppl     1.71\n",
            "| epoch   7 |  2300/  310 batches | lr 0.00 | ms/batch  8.98 | loss  0.52 | ppl     1.68\n",
            "| epoch   7 |  2400/  310 batches | lr 0.00 | ms/batch  9.09 | loss  0.51 | ppl     1.67\n",
            "| epoch   7 |  2500/  310 batches | lr 0.00 | ms/batch  9.01 | loss  0.55 | ppl     1.73\n",
            "| epoch   7 |  2600/  310 batches | lr 0.00 | ms/batch  8.83 | loss  0.51 | ppl     1.67\n",
            "| epoch   7 |  2700/  310 batches | lr 0.00 | ms/batch  8.74 | loss  0.51 | ppl     1.67\n",
            "| epoch   7 |  2800/  310 batches | lr 0.00 | ms/batch  8.82 | loss  0.50 | ppl     1.65\n",
            "| epoch   7 |  2900/  310 batches | lr 0.00 | ms/batch  8.80 | loss  0.54 | ppl     1.71\n",
            "| epoch   7 |  3000/  310 batches | lr 0.00 | ms/batch  8.81 | loss  0.55 | ppl     1.73\n",
            "| epoch   7 |  3100/  310 batches | lr 0.00 | ms/batch  8.67 | loss  0.58 | ppl     1.79\n",
            "| epoch   7 |  3200/  310 batches | lr 0.00 | ms/batch  8.75 | loss  0.56 | ppl     1.76\n",
            "| epoch   7 |  3300/  310 batches | lr 0.00 | ms/batch  8.74 | loss  0.63 | ppl     1.87\n",
            "| epoch   7 |  3400/  310 batches | lr 0.00 | ms/batch  8.70 | loss  0.58 | ppl     1.79\n",
            "| epoch   7 |  3500/  310 batches | lr 0.00 | ms/batch  8.87 | loss  0.60 | ppl     1.83\n",
            "| epoch   7 |  3600/  310 batches | lr 0.00 | ms/batch  8.70 | loss  0.54 | ppl     1.71\n",
            "| epoch   7 |  3700/  310 batches | lr 0.00 | ms/batch  8.67 | loss  0.58 | ppl     1.79\n",
            "| epoch   7 |  3800/  310 batches | lr 0.00 | ms/batch  8.73 | loss  0.58 | ppl     1.79\n",
            "| epoch   7 |  3900/  310 batches | lr 0.00 | ms/batch  9.03 | loss  0.54 | ppl     1.71\n",
            "| epoch   7 |  4000/  310 batches | lr 0.00 | ms/batch  9.10 | loss  0.45 | ppl     1.56\n",
            "| epoch   7 |  4100/  310 batches | lr 0.00 | ms/batch  9.09 | loss  0.41 | ppl     1.51\n",
            "| epoch   7 |  4200/  310 batches | lr 0.00 | ms/batch  8.61 | loss  0.48 | ppl     1.62\n",
            "| epoch   7 |  4300/  310 batches | lr 0.00 | ms/batch  8.75 | loss  0.52 | ppl     1.68\n",
            "| epoch   7 |  4400/  310 batches | lr 0.00 | ms/batch  8.99 | loss  0.58 | ppl     1.78\n",
            "| epoch   7 |  4500/  310 batches | lr 0.00 | ms/batch  9.27 | loss  0.58 | ppl     1.78\n",
            "| epoch   7 |  4600/  310 batches | lr 0.00 | ms/batch  8.91 | loss  0.58 | ppl     1.78\n",
            "| epoch   7 |  4700/  310 batches | lr 0.00 | ms/batch  8.92 | loss  0.56 | ppl     1.75\n",
            "| epoch   7 |  4800/  310 batches | lr 0.00 | ms/batch  8.89 | loss  0.50 | ppl     1.65\n",
            "| epoch   7 |  4900/  310 batches | lr 0.00 | ms/batch  8.64 | loss  0.50 | ppl     1.64\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 44.12s | valid loss  0.46 | valid ppl     1.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |   100/  310 batches | lr 0.00 | ms/batch 10.16 | loss  0.54 | ppl     1.71\n",
            "| epoch   8 |   200/  310 batches | lr 0.00 | ms/batch  8.80 | loss  0.51 | ppl     1.67\n",
            "| epoch   8 |   300/  310 batches | lr 0.00 | ms/batch  8.87 | loss  0.50 | ppl     1.64\n",
            "| epoch   8 |   400/  310 batches | lr 0.00 | ms/batch  8.70 | loss  0.59 | ppl     1.80\n",
            "| epoch   8 |   500/  310 batches | lr 0.00 | ms/batch  8.68 | loss  0.50 | ppl     1.64\n",
            "| epoch   8 |   600/  310 batches | lr 0.00 | ms/batch  8.93 | loss  0.51 | ppl     1.66\n",
            "| epoch   8 |   700/  310 batches | lr 0.00 | ms/batch  9.08 | loss  0.53 | ppl     1.69\n",
            "| epoch   8 |   800/  310 batches | lr 0.00 | ms/batch  9.05 | loss  0.52 | ppl     1.67\n",
            "| epoch   8 |   900/  310 batches | lr 0.00 | ms/batch  9.02 | loss  0.47 | ppl     1.61\n",
            "| epoch   8 |  1000/  310 batches | lr 0.00 | ms/batch  8.57 | loss  0.49 | ppl     1.63\n",
            "| epoch   8 |  1100/  310 batches | lr 0.00 | ms/batch  9.11 | loss  0.47 | ppl     1.60\n",
            "| epoch   8 |  1200/  310 batches | lr 0.00 | ms/batch  8.92 | loss  0.55 | ppl     1.74\n",
            "| epoch   8 |  1300/  310 batches | lr 0.00 | ms/batch 10.11 | loss  0.43 | ppl     1.54\n",
            "| epoch   8 |  1400/  310 batches | lr 0.00 | ms/batch  8.89 | loss  0.46 | ppl     1.58\n",
            "| epoch   8 |  1500/  310 batches | lr 0.00 | ms/batch  8.85 | loss  0.51 | ppl     1.67\n",
            "| epoch   8 |  1600/  310 batches | lr 0.00 | ms/batch  8.74 | loss  0.48 | ppl     1.62\n",
            "| epoch   8 |  1700/  310 batches | lr 0.00 | ms/batch  8.74 | loss  0.47 | ppl     1.60\n",
            "| epoch   8 |  1800/  310 batches | lr 0.00 | ms/batch  8.83 | loss  0.51 | ppl     1.66\n",
            "| epoch   8 |  1900/  310 batches | lr 0.00 | ms/batch  8.96 | loss  0.60 | ppl     1.83\n",
            "| epoch   8 |  2000/  310 batches | lr 0.00 | ms/batch  8.92 | loss  0.49 | ppl     1.64\n",
            "| epoch   8 |  2100/  310 batches | lr 0.00 | ms/batch  8.78 | loss  0.53 | ppl     1.70\n",
            "| epoch   8 |  2200/  310 batches | lr 0.00 | ms/batch  8.78 | loss  0.51 | ppl     1.67\n",
            "| epoch   8 |  2300/  310 batches | lr 0.00 | ms/batch  8.86 | loss  0.49 | ppl     1.64\n",
            "| epoch   8 |  2400/  310 batches | lr 0.00 | ms/batch  8.87 | loss  0.49 | ppl     1.62\n",
            "| epoch   8 |  2500/  310 batches | lr 0.00 | ms/batch  8.87 | loss  0.52 | ppl     1.68\n",
            "| epoch   8 |  2600/  310 batches | lr 0.00 | ms/batch  9.00 | loss  0.49 | ppl     1.63\n",
            "| epoch   8 |  2700/  310 batches | lr 0.00 | ms/batch  8.75 | loss  0.49 | ppl     1.63\n",
            "| epoch   8 |  2800/  310 batches | lr 0.00 | ms/batch  8.98 | loss  0.48 | ppl     1.61\n",
            "| epoch   8 |  2900/  310 batches | lr 0.00 | ms/batch  8.96 | loss  0.51 | ppl     1.67\n",
            "| epoch   8 |  3000/  310 batches | lr 0.00 | ms/batch  8.75 | loss  0.52 | ppl     1.68\n",
            "| epoch   8 |  3100/  310 batches | lr 0.00 | ms/batch  8.58 | loss  0.55 | ppl     1.73\n",
            "| epoch   8 |  3200/  310 batches | lr 0.00 | ms/batch  8.93 | loss  0.53 | ppl     1.70\n",
            "| epoch   8 |  3300/  310 batches | lr 0.00 | ms/batch  8.85 | loss  0.60 | ppl     1.83\n",
            "| epoch   8 |  3400/  310 batches | lr 0.00 | ms/batch  8.66 | loss  0.56 | ppl     1.76\n",
            "| epoch   8 |  3500/  310 batches | lr 0.00 | ms/batch  8.75 | loss  0.59 | ppl     1.80\n",
            "| epoch   8 |  3600/  310 batches | lr 0.00 | ms/batch  8.84 | loss  0.52 | ppl     1.68\n",
            "| epoch   8 |  3700/  310 batches | lr 0.00 | ms/batch  8.97 | loss  0.55 | ppl     1.74\n",
            "| epoch   8 |  3800/  310 batches | lr 0.00 | ms/batch  8.69 | loss  0.56 | ppl     1.75\n",
            "| epoch   8 |  3900/  310 batches | lr 0.00 | ms/batch  8.81 | loss  0.52 | ppl     1.67\n",
            "| epoch   8 |  4000/  310 batches | lr 0.00 | ms/batch  9.05 | loss  0.42 | ppl     1.53\n",
            "| epoch   8 |  4100/  310 batches | lr 0.00 | ms/batch  8.69 | loss  0.39 | ppl     1.48\n",
            "| epoch   8 |  4200/  310 batches | lr 0.00 | ms/batch  8.53 | loss  0.46 | ppl     1.59\n",
            "| epoch   8 |  4300/  310 batches | lr 0.00 | ms/batch  8.75 | loss  0.49 | ppl     1.64\n",
            "| epoch   8 |  4400/  310 batches | lr 0.00 | ms/batch  8.61 | loss  0.55 | ppl     1.74\n",
            "| epoch   8 |  4500/  310 batches | lr 0.00 | ms/batch  8.42 | loss  0.55 | ppl     1.74\n",
            "| epoch   8 |  4600/  310 batches | lr 0.00 | ms/batch  8.54 | loss  0.56 | ppl     1.74\n",
            "| epoch   8 |  4700/  310 batches | lr 0.00 | ms/batch  8.79 | loss  0.53 | ppl     1.70\n",
            "| epoch   8 |  4800/  310 batches | lr 0.00 | ms/batch  8.70 | loss  0.48 | ppl     1.62\n",
            "| epoch   8 |  4900/  310 batches | lr 0.00 | ms/batch  8.79 | loss  0.48 | ppl     1.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 44.24s | valid loss  0.45 | valid ppl     1.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |   100/  310 batches | lr 0.00 | ms/batch  8.87 | loss  0.51 | ppl     1.67\n",
            "| epoch   9 |   200/  310 batches | lr 0.00 | ms/batch  8.49 | loss  0.49 | ppl     1.64\n",
            "| epoch   9 |   300/  310 batches | lr 0.00 | ms/batch  8.74 | loss  0.47 | ppl     1.61\n",
            "| epoch   9 |   400/  310 batches | lr 0.00 | ms/batch  8.96 | loss  0.57 | ppl     1.77\n",
            "| epoch   9 |   500/  310 batches | lr 0.00 | ms/batch  8.71 | loss  0.48 | ppl     1.61\n",
            "| epoch   9 |   600/  310 batches | lr 0.00 | ms/batch  8.66 | loss  0.49 | ppl     1.63\n",
            "| epoch   9 |   700/  310 batches | lr 0.00 | ms/batch  8.54 | loss  0.50 | ppl     1.66\n",
            "| epoch   9 |   800/  310 batches | lr 0.00 | ms/batch  8.63 | loss  0.50 | ppl     1.64\n",
            "| epoch   9 |   900/  310 batches | lr 0.00 | ms/batch  8.76 | loss  0.45 | ppl     1.57\n",
            "| epoch   9 |  1000/  310 batches | lr 0.00 | ms/batch  8.53 | loss  0.47 | ppl     1.59\n",
            "| epoch   9 |  1100/  310 batches | lr 0.00 | ms/batch  8.67 | loss  0.45 | ppl     1.57\n",
            "| epoch   9 |  1200/  310 batches | lr 0.00 | ms/batch  8.59 | loss  0.53 | ppl     1.70\n",
            "| epoch   9 |  1300/  310 batches | lr 0.00 | ms/batch  8.46 | loss  0.42 | ppl     1.52\n",
            "| epoch   9 |  1400/  310 batches | lr 0.00 | ms/batch  9.07 | loss  0.44 | ppl     1.55\n",
            "| epoch   9 |  1500/  310 batches | lr 0.00 | ms/batch  8.51 | loss  0.50 | ppl     1.64\n",
            "| epoch   9 |  1600/  310 batches | lr 0.00 | ms/batch  8.69 | loss  0.46 | ppl     1.59\n",
            "| epoch   9 |  1700/  310 batches | lr 0.00 | ms/batch  8.68 | loss  0.45 | ppl     1.57\n",
            "| epoch   9 |  1800/  310 batches | lr 0.00 | ms/batch  8.70 | loss  0.49 | ppl     1.63\n",
            "| epoch   9 |  1900/  310 batches | lr 0.00 | ms/batch  8.76 | loss  0.59 | ppl     1.80\n",
            "| epoch   9 |  2000/  310 batches | lr 0.00 | ms/batch  9.04 | loss  0.47 | ppl     1.61\n",
            "| epoch   9 |  2100/  310 batches | lr 0.00 | ms/batch  8.52 | loss  0.51 | ppl     1.66\n",
            "| epoch   9 |  2200/  310 batches | lr 0.00 | ms/batch  8.70 | loss  0.49 | ppl     1.64\n",
            "| epoch   9 |  2300/  310 batches | lr 0.00 | ms/batch  8.50 | loss  0.47 | ppl     1.61\n",
            "| epoch   9 |  2400/  310 batches | lr 0.00 | ms/batch  8.43 | loss  0.47 | ppl     1.59\n",
            "| epoch   9 |  2500/  310 batches | lr 0.00 | ms/batch  8.32 | loss  0.50 | ppl     1.65\n",
            "| epoch   9 |  2600/  310 batches | lr 0.00 | ms/batch  8.49 | loss  0.47 | ppl     1.60\n",
            "| epoch   9 |  2700/  310 batches | lr 0.00 | ms/batch  8.39 | loss  0.47 | ppl     1.60\n",
            "| epoch   9 |  2800/  310 batches | lr 0.00 | ms/batch  8.79 | loss  0.46 | ppl     1.58\n",
            "| epoch   9 |  2900/  310 batches | lr 0.00 | ms/batch  8.70 | loss  0.49 | ppl     1.64\n",
            "| epoch   9 |  3000/  310 batches | lr 0.00 | ms/batch  8.67 | loss  0.50 | ppl     1.65\n",
            "| epoch   9 |  3100/  310 batches | lr 0.00 | ms/batch  8.82 | loss  0.52 | ppl     1.69\n",
            "| epoch   9 |  3200/  310 batches | lr 0.00 | ms/batch  8.63 | loss  0.50 | ppl     1.66\n",
            "| epoch   9 |  3300/  310 batches | lr 0.00 | ms/batch  8.64 | loss  0.58 | ppl     1.79\n",
            "| epoch   9 |  3400/  310 batches | lr 0.00 | ms/batch  8.87 | loss  0.55 | ppl     1.73\n",
            "| epoch   9 |  3500/  310 batches | lr 0.00 | ms/batch  8.55 | loss  0.57 | ppl     1.77\n",
            "| epoch   9 |  3600/  310 batches | lr 0.00 | ms/batch  8.67 | loss  0.50 | ppl     1.66\n",
            "| epoch   9 |  3700/  310 batches | lr 0.00 | ms/batch  8.89 | loss  0.54 | ppl     1.72\n",
            "| epoch   9 |  3800/  310 batches | lr 0.00 | ms/batch  8.79 | loss  0.54 | ppl     1.72\n",
            "| epoch   9 |  3900/  310 batches | lr 0.00 | ms/batch  9.00 | loss  0.50 | ppl     1.65\n",
            "| epoch   9 |  4000/  310 batches | lr 0.00 | ms/batch  8.87 | loss  0.41 | ppl     1.51\n",
            "| epoch   9 |  4100/  310 batches | lr 0.00 | ms/batch  8.81 | loss  0.38 | ppl     1.46\n",
            "| epoch   9 |  4200/  310 batches | lr 0.00 | ms/batch  8.69 | loss  0.45 | ppl     1.56\n",
            "| epoch   9 |  4300/  310 batches | lr 0.00 | ms/batch  8.63 | loss  0.48 | ppl     1.61\n",
            "| epoch   9 |  4400/  310 batches | lr 0.00 | ms/batch  9.77 | loss  0.54 | ppl     1.71\n",
            "| epoch   9 |  4500/  310 batches | lr 0.00 | ms/batch 11.49 | loss  0.53 | ppl     1.71\n",
            "| epoch   9 |  4600/  310 batches | lr 0.00 | ms/batch  8.90 | loss  0.54 | ppl     1.71\n",
            "| epoch   9 |  4700/  310 batches | lr 0.00 | ms/batch  8.94 | loss  0.51 | ppl     1.66\n",
            "| epoch   9 |  4800/  310 batches | lr 0.00 | ms/batch  9.90 | loss  0.46 | ppl     1.59\n",
            "| epoch   9 |  4900/  310 batches | lr 0.00 | ms/batch  8.66 | loss  0.46 | ppl     1.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 43.93s | valid loss  0.44 | valid ppl     1.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |   100/  310 batches | lr 0.00 | ms/batch  8.94 | loss  0.50 | ppl     1.64\n",
            "| epoch  10 |   200/  310 batches | lr 0.00 | ms/batch  8.54 | loss  0.48 | ppl     1.61\n",
            "| epoch  10 |   300/  310 batches | lr 0.00 | ms/batch  8.56 | loss  0.46 | ppl     1.58\n",
            "| epoch  10 |   400/  310 batches | lr 0.00 | ms/batch  8.88 | loss  0.56 | ppl     1.74\n",
            "| epoch  10 |   500/  310 batches | lr 0.00 | ms/batch  8.83 | loss  0.46 | ppl     1.59\n",
            "| epoch  10 |   600/  310 batches | lr 0.00 | ms/batch  8.57 | loss  0.47 | ppl     1.61\n",
            "| epoch  10 |   700/  310 batches | lr 0.00 | ms/batch  8.78 | loss  0.49 | ppl     1.64\n",
            "| epoch  10 |   800/  310 batches | lr 0.00 | ms/batch  8.65 | loss  0.48 | ppl     1.62\n",
            "| epoch  10 |   900/  310 batches | lr 0.00 | ms/batch  8.60 | loss  0.43 | ppl     1.54\n",
            "| epoch  10 |  1000/  310 batches | lr 0.00 | ms/batch  8.90 | loss  0.45 | ppl     1.57\n",
            "| epoch  10 |  1100/  310 batches | lr 0.00 | ms/batch  8.93 | loss  0.44 | ppl     1.55\n",
            "| epoch  10 |  1200/  310 batches | lr 0.00 | ms/batch  8.91 | loss  0.51 | ppl     1.67\n",
            "| epoch  10 |  1300/  310 batches | lr 0.00 | ms/batch  8.78 | loss  0.40 | ppl     1.50\n",
            "| epoch  10 |  1400/  310 batches | lr 0.00 | ms/batch  8.92 | loss  0.43 | ppl     1.53\n",
            "| epoch  10 |  1500/  310 batches | lr 0.00 | ms/batch  8.68 | loss  0.48 | ppl     1.62\n",
            "| epoch  10 |  1600/  310 batches | lr 0.00 | ms/batch  8.79 | loss  0.45 | ppl     1.57\n",
            "| epoch  10 |  1700/  310 batches | lr 0.00 | ms/batch  8.81 | loss  0.43 | ppl     1.54\n",
            "| epoch  10 |  1800/  310 batches | lr 0.00 | ms/batch  9.00 | loss  0.48 | ppl     1.61\n",
            "| epoch  10 |  1900/  310 batches | lr 0.00 | ms/batch  8.67 | loss  0.57 | ppl     1.76\n",
            "| epoch  10 |  2000/  310 batches | lr 0.00 | ms/batch 10.09 | loss  0.46 | ppl     1.58\n",
            "| epoch  10 |  2100/  310 batches | lr 0.00 | ms/batch  8.89 | loss  0.49 | ppl     1.63\n",
            "| epoch  10 |  2200/  310 batches | lr 0.00 | ms/batch  8.55 | loss  0.48 | ppl     1.62\n",
            "| epoch  10 |  2300/  310 batches | lr 0.00 | ms/batch  9.62 | loss  0.46 | ppl     1.58\n",
            "| epoch  10 |  2400/  310 batches | lr 0.00 | ms/batch  8.68 | loss  0.46 | ppl     1.58\n",
            "| epoch  10 |  2500/  310 batches | lr 0.00 | ms/batch  8.86 | loss  0.49 | ppl     1.63\n",
            "| epoch  10 |  2600/  310 batches | lr 0.00 | ms/batch  8.53 | loss  0.46 | ppl     1.58\n",
            "| epoch  10 |  2700/  310 batches | lr 0.00 | ms/batch  8.87 | loss  0.46 | ppl     1.58\n",
            "| epoch  10 |  2800/  310 batches | lr 0.00 | ms/batch  8.68 | loss  0.45 | ppl     1.56\n",
            "| epoch  10 |  2900/  310 batches | lr 0.00 | ms/batch  8.79 | loss  0.48 | ppl     1.62\n",
            "| epoch  10 |  3000/  310 batches | lr 0.00 | ms/batch  8.54 | loss  0.48 | ppl     1.62\n",
            "| epoch  10 |  3100/  310 batches | lr 0.00 | ms/batch  8.60 | loss  0.51 | ppl     1.66\n",
            "| epoch  10 |  3200/  310 batches | lr 0.00 | ms/batch  8.92 | loss  0.48 | ppl     1.62\n",
            "| epoch  10 |  3300/  310 batches | lr 0.00 | ms/batch  9.21 | loss  0.57 | ppl     1.77\n",
            "| epoch  10 |  3400/  310 batches | lr 0.00 | ms/batch  8.84 | loss  0.54 | ppl     1.71\n",
            "| epoch  10 |  3500/  310 batches | lr 0.00 | ms/batch  8.70 | loss  0.56 | ppl     1.75\n",
            "| epoch  10 |  3600/  310 batches | lr 0.00 | ms/batch  8.95 | loss  0.50 | ppl     1.64\n",
            "| epoch  10 |  3700/  310 batches | lr 0.00 | ms/batch  8.67 | loss  0.53 | ppl     1.71\n",
            "| epoch  10 |  3800/  310 batches | lr 0.00 | ms/batch  8.77 | loss  0.53 | ppl     1.70\n",
            "| epoch  10 |  3900/  310 batches | lr 0.00 | ms/batch  8.70 | loss  0.49 | ppl     1.63\n",
            "| epoch  10 |  4000/  310 batches | lr 0.00 | ms/batch  8.82 | loss  0.40 | ppl     1.49\n",
            "| epoch  10 |  4100/  310 batches | lr 0.00 | ms/batch  8.84 | loss  0.37 | ppl     1.45\n",
            "| epoch  10 |  4200/  310 batches | lr 0.00 | ms/batch  8.87 | loss  0.43 | ppl     1.54\n",
            "| epoch  10 |  4300/  310 batches | lr 0.00 | ms/batch  8.87 | loss  0.47 | ppl     1.59\n",
            "| epoch  10 |  4400/  310 batches | lr 0.00 | ms/batch  8.72 | loss  0.53 | ppl     1.69\n",
            "| epoch  10 |  4500/  310 batches | lr 0.00 | ms/batch  8.71 | loss  0.52 | ppl     1.69\n",
            "| epoch  10 |  4600/  310 batches | lr 0.00 | ms/batch  8.92 | loss  0.53 | ppl     1.70\n",
            "| epoch  10 |  4700/  310 batches | lr 0.00 | ms/batch  8.92 | loss  0.49 | ppl     1.63\n",
            "| epoch  10 |  4800/  310 batches | lr 0.00 | ms/batch  8.84 | loss  0.45 | ppl     1.57\n",
            "| epoch  10 |  4900/  310 batches | lr 0.00 | ms/batch  8.88 | loss  0.45 | ppl     1.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 44.06s | valid loss  0.43 | valid ppl     1.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  11 |   100/  310 batches | lr 0.00 | ms/batch  9.17 | loss  0.48 | ppl     1.62\n",
            "| epoch  11 |   200/  310 batches | lr 0.00 | ms/batch  8.92 | loss  0.47 | ppl     1.59\n",
            "| epoch  11 |   300/  310 batches | lr 0.00 | ms/batch  9.34 | loss  0.44 | ppl     1.56\n",
            "| epoch  11 |   400/  310 batches | lr 0.00 | ms/batch  8.83 | loss  0.54 | ppl     1.71\n",
            "| epoch  11 |   500/  310 batches | lr 0.00 | ms/batch  9.26 | loss  0.45 | ppl     1.57\n",
            "| epoch  11 |   600/  310 batches | lr 0.00 | ms/batch  9.06 | loss  0.46 | ppl     1.59\n",
            "| epoch  11 |   700/  310 batches | lr 0.00 | ms/batch  8.88 | loss  0.48 | ppl     1.61\n",
            "| epoch  11 |   800/  310 batches | lr 0.00 | ms/batch  8.84 | loss  0.47 | ppl     1.60\n",
            "| epoch  11 |   900/  310 batches | lr 0.00 | ms/batch  9.13 | loss  0.42 | ppl     1.52\n",
            "| epoch  11 |  1000/  310 batches | lr 0.00 | ms/batch  8.66 | loss  0.44 | ppl     1.56\n",
            "| epoch  11 |  1100/  310 batches | lr 0.00 | ms/batch  9.02 | loss  0.42 | ppl     1.53\n",
            "| epoch  11 |  1200/  310 batches | lr 0.00 | ms/batch  8.61 | loss  0.50 | ppl     1.65\n",
            "| epoch  11 |  1300/  310 batches | lr 0.00 | ms/batch  8.64 | loss  0.39 | ppl     1.48\n",
            "| epoch  11 |  1400/  310 batches | lr 0.00 | ms/batch  8.75 | loss  0.42 | ppl     1.52\n",
            "| epoch  11 |  1500/  310 batches | lr 0.00 | ms/batch  9.05 | loss  0.47 | ppl     1.60\n",
            "| epoch  11 |  1600/  310 batches | lr 0.00 | ms/batch  9.15 | loss  0.44 | ppl     1.55\n",
            "| epoch  11 |  1700/  310 batches | lr 0.00 | ms/batch  8.85 | loss  0.42 | ppl     1.53\n",
            "| epoch  11 |  1800/  310 batches | lr 0.00 | ms/batch  8.87 | loss  0.47 | ppl     1.60\n",
            "| epoch  11 |  1900/  310 batches | lr 0.00 | ms/batch  8.74 | loss  0.56 | ppl     1.74\n",
            "| epoch  11 |  2000/  310 batches | lr 0.00 | ms/batch  8.95 | loss  0.45 | ppl     1.56\n",
            "| epoch  11 |  2100/  310 batches | lr 0.00 | ms/batch  8.80 | loss  0.48 | ppl     1.62\n",
            "| epoch  11 |  2200/  310 batches | lr 0.00 | ms/batch  8.86 | loss  0.47 | ppl     1.60\n",
            "| epoch  11 |  2300/  310 batches | lr 0.00 | ms/batch  8.94 | loss  0.45 | ppl     1.56\n",
            "| epoch  11 |  2400/  310 batches | lr 0.00 | ms/batch  8.70 | loss  0.44 | ppl     1.56\n",
            "| epoch  11 |  2500/  310 batches | lr 0.00 | ms/batch  8.77 | loss  0.48 | ppl     1.61\n",
            "| epoch  11 |  2600/  310 batches | lr 0.00 | ms/batch  8.71 | loss  0.45 | ppl     1.57\n",
            "| epoch  11 |  2700/  310 batches | lr 0.00 | ms/batch  8.90 | loss  0.45 | ppl     1.57\n",
            "| epoch  11 |  2800/  310 batches | lr 0.00 | ms/batch  8.66 | loss  0.44 | ppl     1.55\n",
            "| epoch  11 |  2900/  310 batches | lr 0.00 | ms/batch  8.96 | loss  0.47 | ppl     1.60\n",
            "| epoch  11 |  3000/  310 batches | lr 0.00 | ms/batch  8.97 | loss  0.48 | ppl     1.61\n",
            "| epoch  11 |  3100/  310 batches | lr 0.00 | ms/batch  9.07 | loss  0.49 | ppl     1.64\n",
            "| epoch  11 |  3200/  310 batches | lr 0.00 | ms/batch  8.74 | loss  0.47 | ppl     1.61\n",
            "| epoch  11 |  3300/  310 batches | lr 0.00 | ms/batch  8.68 | loss  0.56 | ppl     1.75\n",
            "| epoch  11 |  3400/  310 batches | lr 0.00 | ms/batch  8.70 | loss  0.53 | ppl     1.70\n",
            "| epoch  11 |  3500/  310 batches | lr 0.00 | ms/batch  8.76 | loss  0.55 | ppl     1.74\n",
            "| epoch  11 |  3600/  310 batches | lr 0.00 | ms/batch  8.93 | loss  0.49 | ppl     1.63\n",
            "| epoch  11 |  3700/  310 batches | lr 0.00 | ms/batch  8.69 | loss  0.53 | ppl     1.69\n",
            "| epoch  11 |  3800/  310 batches | lr 0.00 | ms/batch  8.52 | loss  0.53 | ppl     1.69\n",
            "| epoch  11 |  3900/  310 batches | lr 0.00 | ms/batch  8.98 | loss  0.48 | ppl     1.62\n",
            "| epoch  11 |  4000/  310 batches | lr 0.00 | ms/batch  8.68 | loss  0.39 | ppl     1.47\n",
            "| epoch  11 |  4100/  310 batches | lr 0.00 | ms/batch  8.55 | loss  0.36 | ppl     1.44\n",
            "| epoch  11 |  4200/  310 batches | lr 0.00 | ms/batch  9.08 | loss  0.42 | ppl     1.53\n",
            "| epoch  11 |  4300/  310 batches | lr 0.00 | ms/batch  8.70 | loss  0.46 | ppl     1.58\n",
            "| epoch  11 |  4400/  310 batches | lr 0.00 | ms/batch  8.88 | loss  0.52 | ppl     1.68\n",
            "| epoch  11 |  4500/  310 batches | lr 0.00 | ms/batch  8.84 | loss  0.52 | ppl     1.68\n",
            "| epoch  11 |  4600/  310 batches | lr 0.00 | ms/batch  8.84 | loss  0.52 | ppl     1.68\n",
            "| epoch  11 |  4700/  310 batches | lr 0.00 | ms/batch  8.70 | loss  0.48 | ppl     1.61\n",
            "| epoch  11 |  4800/  310 batches | lr 0.00 | ms/batch  8.75 | loss  0.44 | ppl     1.56\n",
            "| epoch  11 |  4900/  310 batches | lr 0.00 | ms/batch  8.96 | loss  0.44 | ppl     1.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time: 44.32s | valid loss  0.43 | valid ppl     1.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  12 |   100/  310 batches | lr 0.00 | ms/batch  9.58 | loss  0.48 | ppl     1.61\n",
            "| epoch  12 |   200/  310 batches | lr 0.00 | ms/batch  8.91 | loss  0.46 | ppl     1.58\n",
            "| epoch  12 |   300/  310 batches | lr 0.00 | ms/batch  8.81 | loss  0.43 | ppl     1.54\n",
            "| epoch  12 |   400/  310 batches | lr 0.00 | ms/batch  8.94 | loss  0.53 | ppl     1.70\n",
            "| epoch  12 |   500/  310 batches | lr 0.00 | ms/batch  9.07 | loss  0.45 | ppl     1.56\n",
            "| epoch  12 |   600/  310 batches | lr 0.00 | ms/batch  8.64 | loss  0.46 | ppl     1.58\n",
            "| epoch  12 |   700/  310 batches | lr 0.00 | ms/batch  8.76 | loss  0.47 | ppl     1.61\n",
            "| epoch  12 |   800/  310 batches | lr 0.00 | ms/batch  8.76 | loss  0.46 | ppl     1.59\n",
            "| epoch  12 |   900/  310 batches | lr 0.00 | ms/batch  8.90 | loss  0.41 | ppl     1.51\n",
            "| epoch  12 |  1000/  310 batches | lr 0.00 | ms/batch  8.86 | loss  0.43 | ppl     1.54\n",
            "| epoch  12 |  1100/  310 batches | lr 0.00 | ms/batch  8.99 | loss  0.42 | ppl     1.52\n",
            "| epoch  12 |  1200/  310 batches | lr 0.00 | ms/batch  8.82 | loss  0.49 | ppl     1.64\n",
            "| epoch  12 |  1300/  310 batches | lr 0.00 | ms/batch  9.11 | loss  0.38 | ppl     1.47\n",
            "| epoch  12 |  1400/  310 batches | lr 0.00 | ms/batch  8.77 | loss  0.41 | ppl     1.51\n",
            "| epoch  12 |  1500/  310 batches | lr 0.00 | ms/batch  8.78 | loss  0.46 | ppl     1.59\n",
            "| epoch  12 |  1600/  310 batches | lr 0.00 | ms/batch  8.84 | loss  0.43 | ppl     1.54\n",
            "| epoch  12 |  1700/  310 batches | lr 0.00 | ms/batch  8.69 | loss  0.41 | ppl     1.51\n",
            "| epoch  12 |  1800/  310 batches | lr 0.00 | ms/batch  8.79 | loss  0.46 | ppl     1.59\n",
            "| epoch  12 |  1900/  310 batches | lr 0.00 | ms/batch  8.79 | loss  0.55 | ppl     1.74\n",
            "| epoch  12 |  2000/  310 batches | lr 0.00 | ms/batch  8.77 | loss  0.44 | ppl     1.55\n",
            "| epoch  12 |  2100/  310 batches | lr 0.00 | ms/batch  9.07 | loss  0.47 | ppl     1.60\n",
            "| epoch  12 |  2200/  310 batches | lr 0.00 | ms/batch  9.04 | loss  0.46 | ppl     1.59\n",
            "| epoch  12 |  2300/  310 batches | lr 0.00 | ms/batch 10.06 | loss  0.44 | ppl     1.55\n",
            "| epoch  12 |  2400/  310 batches | lr 0.00 | ms/batch  9.06 | loss  0.43 | ppl     1.54\n",
            "| epoch  12 |  2500/  310 batches | lr 0.00 | ms/batch  8.85 | loss  0.47 | ppl     1.61\n",
            "| epoch  12 |  2600/  310 batches | lr 0.00 | ms/batch  9.09 | loss  0.44 | ppl     1.55\n",
            "| epoch  12 |  2700/  310 batches | lr 0.00 | ms/batch  9.05 | loss  0.44 | ppl     1.55\n",
            "| epoch  12 |  2800/  310 batches | lr 0.00 | ms/batch  8.94 | loss  0.43 | ppl     1.54\n",
            "| epoch  12 |  2900/  310 batches | lr 0.00 | ms/batch  9.14 | loss  0.46 | ppl     1.59\n",
            "| epoch  12 |  3000/  310 batches | lr 0.00 | ms/batch  9.31 | loss  0.47 | ppl     1.59\n",
            "| epoch  12 |  3100/  310 batches | lr 0.00 | ms/batch  8.79 | loss  0.48 | ppl     1.62\n",
            "| epoch  12 |  3200/  310 batches | lr 0.00 | ms/batch  8.92 | loss  0.46 | ppl     1.59\n",
            "| epoch  12 |  3300/  310 batches | lr 0.00 | ms/batch  8.95 | loss  0.55 | ppl     1.74\n",
            "| epoch  12 |  3400/  310 batches | lr 0.00 | ms/batch  9.02 | loss  0.52 | ppl     1.69\n",
            "| epoch  12 |  3500/  310 batches | lr 0.00 | ms/batch  8.86 | loss  0.55 | ppl     1.73\n",
            "| epoch  12 |  3600/  310 batches | lr 0.00 | ms/batch  8.78 | loss  0.48 | ppl     1.62\n",
            "| epoch  12 |  3700/  310 batches | lr 0.00 | ms/batch  8.93 | loss  0.52 | ppl     1.68\n",
            "| epoch  12 |  3800/  310 batches | lr 0.00 | ms/batch  9.06 | loss  0.52 | ppl     1.69\n",
            "| epoch  12 |  3900/  310 batches | lr 0.00 | ms/batch  8.82 | loss  0.48 | ppl     1.61\n",
            "| epoch  12 |  4000/  310 batches | lr 0.00 | ms/batch  8.69 | loss  0.38 | ppl     1.46\n",
            "| epoch  12 |  4100/  310 batches | lr 0.00 | ms/batch  8.84 | loss  0.35 | ppl     1.42\n",
            "| epoch  12 |  4200/  310 batches | lr 0.00 | ms/batch  8.91 | loss  0.42 | ppl     1.52\n",
            "| epoch  12 |  4300/  310 batches | lr 0.00 | ms/batch  8.95 | loss  0.45 | ppl     1.57\n",
            "| epoch  12 |  4400/  310 batches | lr 0.00 | ms/batch  9.17 | loss  0.51 | ppl     1.67\n",
            "| epoch  12 |  4500/  310 batches | lr 0.00 | ms/batch  8.91 | loss  0.51 | ppl     1.66\n",
            "| epoch  12 |  4600/  310 batches | lr 0.00 | ms/batch  8.80 | loss  0.51 | ppl     1.67\n",
            "| epoch  12 |  4700/  310 batches | lr 0.00 | ms/batch  8.98 | loss  0.47 | ppl     1.60\n",
            "| epoch  12 |  4800/  310 batches | lr 0.00 | ms/batch  8.72 | loss  0.43 | ppl     1.54\n",
            "| epoch  12 |  4900/  310 batches | lr 0.00 | ms/batch  8.67 | loss  0.44 | ppl     1.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time: 44.58s | valid loss  0.42 | valid ppl     1.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  13 |   100/  310 batches | lr 0.00 | ms/batch  8.81 | loss  0.47 | ppl     1.60\n",
            "| epoch  13 |   200/  310 batches | lr 0.00 | ms/batch  9.07 | loss  0.45 | ppl     1.57\n",
            "| epoch  13 |   300/  310 batches | lr 0.00 | ms/batch  8.91 | loss  0.43 | ppl     1.53\n",
            "| epoch  13 |   400/  310 batches | lr 0.00 | ms/batch  9.50 | loss  0.53 | ppl     1.69\n",
            "| epoch  13 |   500/  310 batches | lr 0.00 | ms/batch  8.84 | loss  0.44 | ppl     1.56\n",
            "| epoch  13 |   600/  310 batches | lr 0.00 | ms/batch  8.98 | loss  0.45 | ppl     1.57\n",
            "| epoch  13 |   700/  310 batches | lr 0.00 | ms/batch  8.92 | loss  0.47 | ppl     1.59\n",
            "| epoch  13 |   800/  310 batches | lr 0.00 | ms/batch  8.87 | loss  0.46 | ppl     1.58\n",
            "| epoch  13 |   900/  310 batches | lr 0.00 | ms/batch  8.89 | loss  0.40 | ppl     1.50\n",
            "| epoch  13 |  1000/  310 batches | lr 0.00 | ms/batch  8.80 | loss  0.43 | ppl     1.53\n",
            "| epoch  13 |  1100/  310 batches | lr 0.00 | ms/batch  9.05 | loss  0.41 | ppl     1.51\n",
            "| epoch  13 |  1200/  310 batches | lr 0.00 | ms/batch  8.92 | loss  0.49 | ppl     1.63\n",
            "| epoch  13 |  1300/  310 batches | lr 0.00 | ms/batch  9.10 | loss  0.38 | ppl     1.46\n",
            "| epoch  13 |  1400/  310 batches | lr 0.00 | ms/batch  8.83 | loss  0.40 | ppl     1.49\n",
            "| epoch  13 |  1500/  310 batches | lr 0.00 | ms/batch  9.81 | loss  0.46 | ppl     1.58\n",
            "| epoch  13 |  1600/  310 batches | lr 0.00 | ms/batch  8.61 | loss  0.43 | ppl     1.53\n",
            "| epoch  13 |  1700/  310 batches | lr 0.00 | ms/batch  8.76 | loss  0.40 | ppl     1.50\n",
            "| epoch  13 |  1800/  310 batches | lr 0.00 | ms/batch  8.89 | loss  0.45 | ppl     1.57\n",
            "| epoch  13 |  1900/  310 batches | lr 0.00 | ms/batch  9.09 | loss  0.54 | ppl     1.72\n",
            "| epoch  13 |  2000/  310 batches | lr 0.00 | ms/batch  8.79 | loss  0.43 | ppl     1.54\n",
            "| epoch  13 |  2100/  310 batches | lr 0.00 | ms/batch  8.72 | loss  0.46 | ppl     1.59\n",
            "| epoch  13 |  2200/  310 batches | lr 0.00 | ms/batch  9.30 | loss  0.46 | ppl     1.58\n",
            "| epoch  13 |  2300/  310 batches | lr 0.00 | ms/batch  9.08 | loss  0.43 | ppl     1.54\n",
            "| epoch  13 |  2400/  310 batches | lr 0.00 | ms/batch  8.82 | loss  0.43 | ppl     1.53\n",
            "| epoch  13 |  2500/  310 batches | lr 0.00 | ms/batch  8.75 | loss  0.47 | ppl     1.59\n",
            "| epoch  13 |  2600/  310 batches | lr 0.00 | ms/batch  9.21 | loss  0.43 | ppl     1.54\n",
            "| epoch  13 |  2700/  310 batches | lr 0.00 | ms/batch  8.78 | loss  0.44 | ppl     1.55\n",
            "| epoch  13 |  2800/  310 batches | lr 0.00 | ms/batch  8.70 | loss  0.42 | ppl     1.52\n",
            "| epoch  13 |  2900/  310 batches | lr 0.00 | ms/batch  8.79 | loss  0.46 | ppl     1.58\n",
            "| epoch  13 |  3000/  310 batches | lr 0.00 | ms/batch  8.92 | loss  0.46 | ppl     1.58\n",
            "| epoch  13 |  3100/  310 batches | lr 0.00 | ms/batch  9.08 | loss  0.48 | ppl     1.62\n",
            "| epoch  13 |  3200/  310 batches | lr 0.00 | ms/batch  9.21 | loss  0.45 | ppl     1.58\n",
            "| epoch  13 |  3300/  310 batches | lr 0.00 | ms/batch  9.46 | loss  0.54 | ppl     1.72\n",
            "| epoch  13 |  3400/  310 batches | lr 0.00 | ms/batch  8.92 | loss  0.52 | ppl     1.68\n",
            "| epoch  13 |  3500/  310 batches | lr 0.00 | ms/batch  9.05 | loss  0.54 | ppl     1.72\n",
            "| epoch  13 |  3600/  310 batches | lr 0.00 | ms/batch  9.21 | loss  0.47 | ppl     1.61\n",
            "| epoch  13 |  3700/  310 batches | lr 0.00 | ms/batch  8.91 | loss  0.51 | ppl     1.67\n",
            "| epoch  13 |  3800/  310 batches | lr 0.00 | ms/batch  8.96 | loss  0.52 | ppl     1.67\n",
            "| epoch  13 |  3900/  310 batches | lr 0.00 | ms/batch  8.82 | loss  0.47 | ppl     1.60\n",
            "| epoch  13 |  4000/  310 batches | lr 0.00 | ms/batch  9.45 | loss  0.38 | ppl     1.46\n",
            "| epoch  13 |  4100/  310 batches | lr 0.00 | ms/batch  9.21 | loss  0.35 | ppl     1.42\n",
            "| epoch  13 |  4200/  310 batches | lr 0.00 | ms/batch  8.77 | loss  0.41 | ppl     1.51\n",
            "| epoch  13 |  4300/  310 batches | lr 0.00 | ms/batch  8.79 | loss  0.44 | ppl     1.56\n",
            "| epoch  13 |  4400/  310 batches | lr 0.00 | ms/batch  8.75 | loss  0.50 | ppl     1.65\n",
            "| epoch  13 |  4500/  310 batches | lr 0.00 | ms/batch  8.77 | loss  0.50 | ppl     1.65\n",
            "| epoch  13 |  4600/  310 batches | lr 0.00 | ms/batch  8.75 | loss  0.50 | ppl     1.65\n",
            "| epoch  13 |  4700/  310 batches | lr 0.00 | ms/batch  8.70 | loss  0.46 | ppl     1.58\n",
            "| epoch  13 |  4800/  310 batches | lr 0.00 | ms/batch  9.01 | loss  0.43 | ppl     1.54\n",
            "| epoch  13 |  4900/  310 batches | lr 0.00 | ms/batch  9.39 | loss  0.43 | ppl     1.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time: 44.78s | valid loss  0.42 | valid ppl     1.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  14 |   100/  310 batches | lr 0.00 | ms/batch  8.83 | loss  0.46 | ppl     1.59\n",
            "| epoch  14 |   200/  310 batches | lr 0.00 | ms/batch  8.81 | loss  0.44 | ppl     1.56\n",
            "| epoch  14 |   300/  310 batches | lr 0.00 | ms/batch  9.27 | loss  0.42 | ppl     1.52\n",
            "| epoch  14 |   400/  310 batches | lr 0.00 | ms/batch  9.18 | loss  0.52 | ppl     1.68\n",
            "| epoch  14 |   500/  310 batches | lr 0.00 | ms/batch  8.88 | loss  0.44 | ppl     1.55\n",
            "| epoch  14 |   600/  310 batches | lr 0.00 | ms/batch  8.78 | loss  0.45 | ppl     1.56\n",
            "| epoch  14 |   700/  310 batches | lr 0.00 | ms/batch  8.67 | loss  0.46 | ppl     1.59\n",
            "| epoch  14 |   800/  310 batches | lr 0.00 | ms/batch  8.64 | loss  0.45 | ppl     1.57\n",
            "| epoch  14 |   900/  310 batches | lr 0.00 | ms/batch  8.73 | loss  0.40 | ppl     1.49\n",
            "| epoch  14 |  1000/  310 batches | lr 0.00 | ms/batch  8.84 | loss  0.42 | ppl     1.52\n",
            "| epoch  14 |  1100/  310 batches | lr 0.00 | ms/batch  8.79 | loss  0.40 | ppl     1.50\n",
            "| epoch  14 |  1200/  310 batches | lr 0.00 | ms/batch  9.03 | loss  0.48 | ppl     1.62\n",
            "| epoch  14 |  1300/  310 batches | lr 0.00 | ms/batch  8.85 | loss  0.38 | ppl     1.46\n",
            "| epoch  14 |  1400/  310 batches | lr 0.00 | ms/batch  9.05 | loss  0.39 | ppl     1.48\n",
            "| epoch  14 |  1500/  310 batches | lr 0.00 | ms/batch  9.01 | loss  0.45 | ppl     1.58\n",
            "| epoch  14 |  1600/  310 batches | lr 0.00 | ms/batch  8.58 | loss  0.42 | ppl     1.52\n",
            "| epoch  14 |  1700/  310 batches | lr 0.00 | ms/batch  8.77 | loss  0.40 | ppl     1.49\n",
            "| epoch  14 |  1800/  310 batches | lr 0.00 | ms/batch  8.70 | loss  0.45 | ppl     1.57\n",
            "| epoch  14 |  1900/  310 batches | lr 0.00 | ms/batch  8.90 | loss  0.54 | ppl     1.71\n",
            "| epoch  14 |  2000/  310 batches | lr 0.00 | ms/batch  8.75 | loss  0.43 | ppl     1.53\n",
            "| epoch  14 |  2100/  310 batches | lr 0.00 | ms/batch  9.17 | loss  0.46 | ppl     1.58\n",
            "| epoch  14 |  2200/  310 batches | lr 0.00 | ms/batch  8.77 | loss  0.45 | ppl     1.57\n",
            "| epoch  14 |  2300/  310 batches | lr 0.00 | ms/batch  8.66 | loss  0.43 | ppl     1.53\n",
            "| epoch  14 |  2400/  310 batches | lr 0.00 | ms/batch  8.85 | loss  0.42 | ppl     1.53\n",
            "| epoch  14 |  2500/  310 batches | lr 0.00 | ms/batch  8.90 | loss  0.46 | ppl     1.59\n",
            "| epoch  14 |  2600/  310 batches | lr 0.00 | ms/batch  8.65 | loss  0.43 | ppl     1.54\n",
            "| epoch  14 |  2700/  310 batches | lr 0.00 | ms/batch  9.03 | loss  0.43 | ppl     1.54\n",
            "| epoch  14 |  2800/  310 batches | lr 0.00 | ms/batch  8.71 | loss  0.42 | ppl     1.52\n",
            "| epoch  14 |  2900/  310 batches | lr 0.00 | ms/batch  8.77 | loss  0.45 | ppl     1.57\n",
            "| epoch  14 |  3000/  310 batches | lr 0.00 | ms/batch  9.51 | loss  0.46 | ppl     1.58\n",
            "| epoch  14 |  3100/  310 batches | lr 0.00 | ms/batch  9.06 | loss  0.47 | ppl     1.60\n",
            "| epoch  14 |  3200/  310 batches | lr 0.00 | ms/batch  8.98 | loss  0.45 | ppl     1.56\n",
            "| epoch  14 |  3300/  310 batches | lr 0.00 | ms/batch  8.71 | loss  0.54 | ppl     1.72\n",
            "| epoch  14 |  3400/  310 batches | lr 0.00 | ms/batch  8.75 | loss  0.52 | ppl     1.67\n",
            "| epoch  14 |  3500/  310 batches | lr 0.00 | ms/batch  8.92 | loss  0.54 | ppl     1.71\n",
            "| epoch  14 |  3600/  310 batches | lr 0.00 | ms/batch  9.10 | loss  0.47 | ppl     1.60\n",
            "| epoch  14 |  3700/  310 batches | lr 0.00 | ms/batch  8.92 | loss  0.51 | ppl     1.66\n",
            "| epoch  14 |  3800/  310 batches | lr 0.00 | ms/batch  8.62 | loss  0.51 | ppl     1.67\n",
            "| epoch  14 |  3900/  310 batches | lr 0.00 | ms/batch  9.48 | loss  0.47 | ppl     1.59\n",
            "| epoch  14 |  4000/  310 batches | lr 0.00 | ms/batch  8.78 | loss  0.37 | ppl     1.45\n",
            "| epoch  14 |  4100/  310 batches | lr 0.00 | ms/batch  8.96 | loss  0.35 | ppl     1.41\n",
            "| epoch  14 |  4200/  310 batches | lr 0.00 | ms/batch  9.21 | loss  0.41 | ppl     1.50\n",
            "| epoch  14 |  4300/  310 batches | lr 0.00 | ms/batch  8.80 | loss  0.44 | ppl     1.55\n",
            "| epoch  14 |  4400/  310 batches | lr 0.00 | ms/batch  8.86 | loss  0.50 | ppl     1.65\n",
            "| epoch  14 |  4500/  310 batches | lr 0.00 | ms/batch  9.08 | loss  0.50 | ppl     1.64\n",
            "| epoch  14 |  4600/  310 batches | lr 0.00 | ms/batch  8.78 | loss  0.50 | ppl     1.65\n",
            "| epoch  14 |  4700/  310 batches | lr 0.00 | ms/batch  8.76 | loss  0.45 | ppl     1.57\n",
            "| epoch  14 |  4800/  310 batches | lr 0.00 | ms/batch  8.76 | loss  0.43 | ppl     1.53\n",
            "| epoch  14 |  4900/  310 batches | lr 0.00 | ms/batch  9.10 | loss  0.42 | ppl     1.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time: 44.40s | valid loss  0.42 | valid ppl     1.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  15 |   100/  310 batches | lr 0.00 | ms/batch  8.97 | loss  0.46 | ppl     1.58\n",
            "| epoch  15 |   200/  310 batches | lr 0.00 | ms/batch  8.73 | loss  0.44 | ppl     1.55\n",
            "| epoch  15 |   300/  310 batches | lr 0.00 | ms/batch  9.49 | loss  0.42 | ppl     1.51\n",
            "| epoch  15 |   400/  310 batches | lr 0.00 | ms/batch  9.03 | loss  0.52 | ppl     1.68\n",
            "| epoch  15 |   500/  310 batches | lr 0.00 | ms/batch  9.13 | loss  0.43 | ppl     1.54\n",
            "| epoch  15 |   600/  310 batches | lr 0.00 | ms/batch  8.98 | loss  0.44 | ppl     1.56\n",
            "| epoch  15 |   700/  310 batches | lr 0.00 | ms/batch  8.92 | loss  0.46 | ppl     1.58\n",
            "| epoch  15 |   800/  310 batches | lr 0.00 | ms/batch  8.87 | loss  0.45 | ppl     1.57\n",
            "| epoch  15 |   900/  310 batches | lr 0.00 | ms/batch  9.03 | loss  0.39 | ppl     1.48\n",
            "| epoch  15 |  1000/  310 batches | lr 0.00 | ms/batch  8.67 | loss  0.42 | ppl     1.52\n",
            "| epoch  15 |  1100/  310 batches | lr 0.00 | ms/batch  8.77 | loss  0.40 | ppl     1.49\n",
            "| epoch  15 |  1200/  310 batches | lr 0.00 | ms/batch  8.66 | loss  0.48 | ppl     1.61\n",
            "| epoch  15 |  1300/  310 batches | lr 0.00 | ms/batch  8.83 | loss  0.37 | ppl     1.45\n",
            "| epoch  15 |  1400/  310 batches | lr 0.00 | ms/batch  8.86 | loss  0.39 | ppl     1.48\n",
            "| epoch  15 |  1500/  310 batches | lr 0.00 | ms/batch  8.81 | loss  0.45 | ppl     1.57\n",
            "| epoch  15 |  1600/  310 batches | lr 0.00 | ms/batch  8.81 | loss  0.42 | ppl     1.52\n",
            "| epoch  15 |  1700/  310 batches | lr 0.00 | ms/batch  8.93 | loss  0.39 | ppl     1.48\n",
            "| epoch  15 |  1800/  310 batches | lr 0.00 | ms/batch  8.63 | loss  0.45 | ppl     1.56\n",
            "| epoch  15 |  1900/  310 batches | lr 0.00 | ms/batch  8.86 | loss  0.53 | ppl     1.71\n",
            "| epoch  15 |  2000/  310 batches | lr 0.00 | ms/batch  8.95 | loss  0.42 | ppl     1.53\n",
            "| epoch  15 |  2100/  310 batches | lr 0.00 | ms/batch  8.86 | loss  0.45 | ppl     1.57\n",
            "| epoch  15 |  2200/  310 batches | lr 0.00 | ms/batch  8.94 | loss  0.45 | ppl     1.57\n",
            "| epoch  15 |  2300/  310 batches | lr 0.00 | ms/batch  9.08 | loss  0.42 | ppl     1.52\n",
            "| epoch  15 |  2400/  310 batches | lr 0.00 | ms/batch  8.79 | loss  0.42 | ppl     1.52\n",
            "| epoch  15 |  2500/  310 batches | lr 0.00 | ms/batch  8.84 | loss  0.46 | ppl     1.58\n",
            "| epoch  15 |  2600/  310 batches | lr 0.00 | ms/batch  8.91 | loss  0.43 | ppl     1.53\n",
            "| epoch  15 |  2700/  310 batches | lr 0.00 | ms/batch  8.99 | loss  0.43 | ppl     1.53\n",
            "| epoch  15 |  2800/  310 batches | lr 0.00 | ms/batch  8.76 | loss  0.41 | ppl     1.51\n",
            "| epoch  15 |  2900/  310 batches | lr 0.00 | ms/batch  8.85 | loss  0.45 | ppl     1.57\n",
            "| epoch  15 |  3000/  310 batches | lr 0.00 | ms/batch  8.95 | loss  0.45 | ppl     1.57\n",
            "| epoch  15 |  3100/  310 batches | lr 0.00 | ms/batch  8.58 | loss  0.47 | ppl     1.59\n",
            "| epoch  15 |  3200/  310 batches | lr 0.00 | ms/batch  8.90 | loss  0.44 | ppl     1.55\n",
            "| epoch  15 |  3300/  310 batches | lr 0.00 | ms/batch  8.71 | loss  0.53 | ppl     1.71\n",
            "| epoch  15 |  3400/  310 batches | lr 0.00 | ms/batch  8.86 | loss  0.51 | ppl     1.67\n",
            "| epoch  15 |  3500/  310 batches | lr 0.00 | ms/batch  9.16 | loss  0.54 | ppl     1.71\n",
            "| epoch  15 |  3600/  310 batches | lr 0.00 | ms/batch  8.72 | loss  0.47 | ppl     1.59\n",
            "| epoch  15 |  3700/  310 batches | lr 0.00 | ms/batch  8.68 | loss  0.51 | ppl     1.66\n",
            "| epoch  15 |  3800/  310 batches | lr 0.00 | ms/batch  8.70 | loss  0.51 | ppl     1.66\n",
            "| epoch  15 |  3900/  310 batches | lr 0.00 | ms/batch  8.91 | loss  0.46 | ppl     1.59\n",
            "| epoch  15 |  4000/  310 batches | lr 0.00 | ms/batch  8.80 | loss  0.37 | ppl     1.44\n",
            "| epoch  15 |  4100/  310 batches | lr 0.00 | ms/batch  9.29 | loss  0.34 | ppl     1.41\n",
            "| epoch  15 |  4200/  310 batches | lr 0.00 | ms/batch  8.81 | loss  0.40 | ppl     1.49\n",
            "| epoch  15 |  4300/  310 batches | lr 0.00 | ms/batch  8.72 | loss  0.43 | ppl     1.54\n",
            "| epoch  15 |  4400/  310 batches | lr 0.00 | ms/batch  8.71 | loss  0.50 | ppl     1.64\n",
            "| epoch  15 |  4500/  310 batches | lr 0.00 | ms/batch  8.69 | loss  0.49 | ppl     1.64\n",
            "| epoch  15 |  4600/  310 batches | lr 0.00 | ms/batch  8.74 | loss  0.50 | ppl     1.64\n",
            "| epoch  15 |  4700/  310 batches | lr 0.00 | ms/batch  8.80 | loss  0.45 | ppl     1.56\n",
            "| epoch  15 |  4800/  310 batches | lr 0.00 | ms/batch  8.54 | loss  0.42 | ppl     1.52\n",
            "| epoch  15 |  4900/  310 batches | lr 0.00 | ms/batch  8.73 | loss  0.42 | ppl     1.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time: 44.16s | valid loss  0.42 | valid ppl     1.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  16 |   100/  310 batches | lr 0.00 | ms/batch  8.70 | loss  0.45 | ppl     1.57\n",
            "| epoch  16 |   200/  310 batches | lr 0.00 | ms/batch  8.79 | loss  0.43 | ppl     1.54\n",
            "| epoch  16 |   300/  310 batches | lr 0.00 | ms/batch  8.77 | loss  0.41 | ppl     1.51\n",
            "| epoch  16 |   400/  310 batches | lr 0.00 | ms/batch  9.48 | loss  0.51 | ppl     1.66\n",
            "| epoch  16 |   500/  310 batches | lr 0.00 | ms/batch  8.60 | loss  0.43 | ppl     1.54\n",
            "| epoch  16 |   600/  310 batches | lr 0.00 | ms/batch  9.13 | loss  0.44 | ppl     1.55\n",
            "| epoch  16 |   700/  310 batches | lr 0.00 | ms/batch  9.35 | loss  0.46 | ppl     1.58\n",
            "| epoch  16 |   800/  310 batches | lr 0.00 | ms/batch  8.89 | loss  0.44 | ppl     1.56\n",
            "| epoch  16 |   900/  310 batches | lr 0.00 | ms/batch  8.89 | loss  0.39 | ppl     1.48\n",
            "| epoch  16 |  1000/  310 batches | lr 0.00 | ms/batch  8.97 | loss  0.41 | ppl     1.51\n",
            "| epoch  16 |  1100/  310 batches | lr 0.00 | ms/batch  8.57 | loss  0.39 | ppl     1.48\n",
            "| epoch  16 |  1200/  310 batches | lr 0.00 | ms/batch  8.69 | loss  0.47 | ppl     1.61\n",
            "| epoch  16 |  1300/  310 batches | lr 0.00 | ms/batch  8.90 | loss  0.37 | ppl     1.45\n",
            "| epoch  16 |  1400/  310 batches | lr 0.00 | ms/batch  9.24 | loss  0.39 | ppl     1.48\n",
            "| epoch  16 |  1500/  310 batches | lr 0.00 | ms/batch  8.84 | loss  0.45 | ppl     1.56\n",
            "| epoch  16 |  1600/  310 batches | lr 0.00 | ms/batch  8.86 | loss  0.41 | ppl     1.51\n",
            "| epoch  16 |  1700/  310 batches | lr 0.00 | ms/batch  8.78 | loss  0.39 | ppl     1.48\n",
            "| epoch  16 |  1800/  310 batches | lr 0.00 | ms/batch  9.00 | loss  0.44 | ppl     1.56\n",
            "| epoch  16 |  1900/  310 batches | lr 0.00 | ms/batch  8.52 | loss  0.53 | ppl     1.70\n",
            "| epoch  16 |  2000/  310 batches | lr 0.00 | ms/batch  8.60 | loss  0.42 | ppl     1.52\n",
            "| epoch  16 |  2100/  310 batches | lr 0.00 | ms/batch  8.73 | loss  0.45 | ppl     1.57\n",
            "| epoch  16 |  2200/  310 batches | lr 0.00 | ms/batch  8.96 | loss  0.44 | ppl     1.56\n",
            "| epoch  16 |  2300/  310 batches | lr 0.00 | ms/batch  8.64 | loss  0.42 | ppl     1.52\n",
            "| epoch  16 |  2400/  310 batches | lr 0.00 | ms/batch  8.76 | loss  0.42 | ppl     1.52\n",
            "| epoch  16 |  2500/  310 batches | lr 0.00 | ms/batch  8.58 | loss  0.45 | ppl     1.57\n",
            "| epoch  16 |  2600/  310 batches | lr 0.00 | ms/batch  8.84 | loss  0.42 | ppl     1.53\n",
            "| epoch  16 |  2700/  310 batches | lr 0.00 | ms/batch  8.51 | loss  0.43 | ppl     1.53\n",
            "| epoch  16 |  2800/  310 batches | lr 0.00 | ms/batch  8.57 | loss  0.41 | ppl     1.50\n",
            "| epoch  16 |  2900/  310 batches | lr 0.00 | ms/batch  8.70 | loss  0.45 | ppl     1.56\n",
            "| epoch  16 |  3000/  310 batches | lr 0.00 | ms/batch  8.88 | loss  0.45 | ppl     1.56\n",
            "| epoch  16 |  3100/  310 batches | lr 0.00 | ms/batch  8.56 | loss  0.47 | ppl     1.59\n",
            "| epoch  16 |  3200/  310 batches | lr 0.00 | ms/batch  8.78 | loss  0.44 | ppl     1.55\n",
            "| epoch  16 |  3300/  310 batches | lr 0.00 | ms/batch  8.98 | loss  0.53 | ppl     1.71\n",
            "| epoch  16 |  3400/  310 batches | lr 0.00 | ms/batch  8.66 | loss  0.51 | ppl     1.66\n",
            "| epoch  16 |  3500/  310 batches | lr 0.00 | ms/batch  8.42 | loss  0.53 | ppl     1.71\n",
            "| epoch  16 |  3600/  310 batches | lr 0.00 | ms/batch  8.92 | loss  0.46 | ppl     1.59\n",
            "| epoch  16 |  3700/  310 batches | lr 0.00 | ms/batch  8.96 | loss  0.50 | ppl     1.65\n",
            "| epoch  16 |  3800/  310 batches | lr 0.00 | ms/batch  8.99 | loss  0.50 | ppl     1.66\n",
            "| epoch  16 |  3900/  310 batches | lr 0.00 | ms/batch  8.65 | loss  0.46 | ppl     1.58\n",
            "| epoch  16 |  4000/  310 batches | lr 0.00 | ms/batch  8.49 | loss  0.36 | ppl     1.44\n",
            "| epoch  16 |  4100/  310 batches | lr 0.00 | ms/batch  8.66 | loss  0.34 | ppl     1.41\n",
            "| epoch  16 |  4200/  310 batches | lr 0.00 | ms/batch  8.90 | loss  0.40 | ppl     1.49\n",
            "| epoch  16 |  4300/  310 batches | lr 0.00 | ms/batch  8.63 | loss  0.43 | ppl     1.54\n",
            "| epoch  16 |  4400/  310 batches | lr 0.00 | ms/batch  8.60 | loss  0.49 | ppl     1.64\n",
            "| epoch  16 |  4500/  310 batches | lr 0.00 | ms/batch  8.63 | loss  0.49 | ppl     1.63\n",
            "| epoch  16 |  4600/  310 batches | lr 0.00 | ms/batch  9.06 | loss  0.49 | ppl     1.64\n",
            "| epoch  16 |  4700/  310 batches | lr 0.00 | ms/batch  8.79 | loss  0.44 | ppl     1.56\n",
            "| epoch  16 |  4800/  310 batches | lr 0.00 | ms/batch  8.94 | loss  0.42 | ppl     1.52\n",
            "| epoch  16 |  4900/  310 batches | lr 0.00 | ms/batch  8.81 | loss  0.42 | ppl     1.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time: 43.96s | valid loss  0.41 | valid ppl     1.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  17 |   100/  310 batches | lr 0.00 | ms/batch  8.87 | loss  0.45 | ppl     1.57\n",
            "| epoch  17 |   200/  310 batches | lr 0.00 | ms/batch  9.17 | loss  0.43 | ppl     1.54\n",
            "| epoch  17 |   300/  310 batches | lr 0.00 | ms/batch  8.87 | loss  0.41 | ppl     1.50\n",
            "| epoch  17 |   400/  310 batches | lr 0.00 | ms/batch  9.26 | loss  0.51 | ppl     1.66\n",
            "| epoch  17 |   500/  310 batches | lr 0.00 | ms/batch  8.66 | loss  0.42 | ppl     1.53\n",
            "| epoch  17 |   600/  310 batches | lr 0.00 | ms/batch  8.78 | loss  0.43 | ppl     1.54\n",
            "| epoch  17 |   700/  310 batches | lr 0.00 | ms/batch  8.69 | loss  0.45 | ppl     1.57\n",
            "| epoch  17 |   800/  310 batches | lr 0.00 | ms/batch  9.06 | loss  0.44 | ppl     1.55\n",
            "| epoch  17 |   900/  310 batches | lr 0.00 | ms/batch  8.96 | loss  0.39 | ppl     1.47\n",
            "| epoch  17 |  1000/  310 batches | lr 0.00 | ms/batch  8.95 | loss  0.41 | ppl     1.51\n",
            "| epoch  17 |  1100/  310 batches | lr 0.00 | ms/batch  9.06 | loss  0.39 | ppl     1.48\n",
            "| epoch  17 |  1200/  310 batches | lr 0.00 | ms/batch  9.04 | loss  0.47 | ppl     1.60\n",
            "| epoch  17 |  1300/  310 batches | lr 0.00 | ms/batch  8.86 | loss  0.36 | ppl     1.44\n",
            "| epoch  17 |  1400/  310 batches | lr 0.00 | ms/batch  8.77 | loss  0.39 | ppl     1.47\n",
            "| epoch  17 |  1500/  310 batches | lr 0.00 | ms/batch  9.02 | loss  0.44 | ppl     1.56\n",
            "| epoch  17 |  1600/  310 batches | lr 0.00 | ms/batch  8.84 | loss  0.41 | ppl     1.51\n",
            "| epoch  17 |  1700/  310 batches | lr 0.00 | ms/batch  9.08 | loss  0.39 | ppl     1.47\n",
            "| epoch  17 |  1800/  310 batches | lr 0.00 | ms/batch  8.99 | loss  0.44 | ppl     1.56\n",
            "| epoch  17 |  1900/  310 batches | lr 0.00 | ms/batch  8.80 | loss  0.53 | ppl     1.69\n",
            "| epoch  17 |  2000/  310 batches | lr 0.00 | ms/batch  8.64 | loss  0.42 | ppl     1.52\n",
            "| epoch  17 |  2100/  310 batches | lr 0.00 | ms/batch  8.77 | loss  0.45 | ppl     1.56\n",
            "| epoch  17 |  2200/  310 batches | lr 0.00 | ms/batch  8.87 | loss  0.44 | ppl     1.56\n",
            "| epoch  17 |  2300/  310 batches | lr 0.00 | ms/batch  9.07 | loss  0.41 | ppl     1.51\n",
            "| epoch  17 |  2400/  310 batches | lr 0.00 | ms/batch  9.12 | loss  0.41 | ppl     1.51\n",
            "| epoch  17 |  2500/  310 batches | lr 0.00 | ms/batch  8.72 | loss  0.45 | ppl     1.57\n",
            "| epoch  17 |  2600/  310 batches | lr 0.00 | ms/batch  9.15 | loss  0.42 | ppl     1.52\n",
            "| epoch  17 |  2700/  310 batches | lr 0.00 | ms/batch  8.96 | loss  0.42 | ppl     1.52\n",
            "| epoch  17 |  2800/  310 batches | lr 0.00 | ms/batch  8.72 | loss  0.41 | ppl     1.50\n",
            "| epoch  17 |  2900/  310 batches | lr 0.00 | ms/batch  8.57 | loss  0.44 | ppl     1.56\n",
            "| epoch  17 |  3000/  310 batches | lr 0.00 | ms/batch  8.81 | loss  0.45 | ppl     1.56\n",
            "| epoch  17 |  3100/  310 batches | lr 0.00 | ms/batch  9.00 | loss  0.46 | ppl     1.58\n",
            "| epoch  17 |  3200/  310 batches | lr 0.00 | ms/batch  9.04 | loss  0.43 | ppl     1.54\n",
            "| epoch  17 |  3300/  310 batches | lr 0.00 | ms/batch  8.94 | loss  0.53 | ppl     1.70\n",
            "| epoch  17 |  3400/  310 batches | lr 0.00 | ms/batch  8.67 | loss  0.51 | ppl     1.66\n",
            "| epoch  17 |  3500/  310 batches | lr 0.00 | ms/batch  9.17 | loss  0.53 | ppl     1.70\n",
            "| epoch  17 |  3600/  310 batches | lr 0.00 | ms/batch  8.58 | loss  0.46 | ppl     1.59\n",
            "| epoch  17 |  3700/  310 batches | lr 0.00 | ms/batch  8.46 | loss  0.50 | ppl     1.65\n",
            "| epoch  17 |  3800/  310 batches | lr 0.00 | ms/batch  8.47 | loss  0.50 | ppl     1.65\n",
            "| epoch  17 |  3900/  310 batches | lr 0.00 | ms/batch  8.17 | loss  0.46 | ppl     1.58\n",
            "| epoch  17 |  4000/  310 batches | lr 0.00 | ms/batch  8.14 | loss  0.36 | ppl     1.44\n",
            "| epoch  17 |  4100/  310 batches | lr 0.00 | ms/batch  8.55 | loss  0.34 | ppl     1.40\n",
            "| epoch  17 |  4200/  310 batches | lr 0.00 | ms/batch  8.43 | loss  0.40 | ppl     1.49\n",
            "| epoch  17 |  4300/  310 batches | lr 0.00 | ms/batch  8.41 | loss  0.43 | ppl     1.53\n",
            "| epoch  17 |  4400/  310 batches | lr 0.00 | ms/batch  8.29 | loss  0.49 | ppl     1.64\n",
            "| epoch  17 |  4500/  310 batches | lr 0.00 | ms/batch  8.31 | loss  0.49 | ppl     1.62\n",
            "| epoch  17 |  4600/  310 batches | lr 0.00 | ms/batch  8.06 | loss  0.49 | ppl     1.63\n",
            "| epoch  17 |  4700/  310 batches | lr 0.00 | ms/batch  8.82 | loss  0.44 | ppl     1.55\n",
            "| epoch  17 |  4800/  310 batches | lr 0.00 | ms/batch  8.11 | loss  0.42 | ppl     1.52\n",
            "| epoch  17 |  4900/  310 batches | lr 0.00 | ms/batch  8.28 | loss  0.41 | ppl     1.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time: 43.66s | valid loss  0.41 | valid ppl     1.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  18 |   100/  310 batches | lr 0.00 | ms/batch  8.43 | loss  0.45 | ppl     1.56\n",
            "| epoch  18 |   200/  310 batches | lr 0.00 | ms/batch  8.07 | loss  0.43 | ppl     1.54\n",
            "| epoch  18 |   300/  310 batches | lr 0.00 | ms/batch  8.61 | loss  0.40 | ppl     1.50\n",
            "| epoch  18 |   400/  310 batches | lr 0.00 | ms/batch  8.12 | loss  0.51 | ppl     1.66\n",
            "| epoch  18 |   500/  310 batches | lr 0.00 | ms/batch  8.66 | loss  0.42 | ppl     1.53\n",
            "| epoch  18 |   600/  310 batches | lr 0.00 | ms/batch  8.17 | loss  0.43 | ppl     1.54\n",
            "| epoch  18 |   700/  310 batches | lr 0.00 | ms/batch  8.02 | loss  0.45 | ppl     1.57\n",
            "| epoch  18 |   800/  310 batches | lr 0.00 | ms/batch  8.15 | loss  0.44 | ppl     1.55\n",
            "| epoch  18 |   900/  310 batches | lr 0.00 | ms/batch  8.56 | loss  0.38 | ppl     1.47\n",
            "| epoch  18 |  1000/  310 batches | lr 0.00 | ms/batch  8.13 | loss  0.41 | ppl     1.50\n",
            "| epoch  18 |  1100/  310 batches | lr 0.00 | ms/batch  8.40 | loss  0.39 | ppl     1.47\n",
            "| epoch  18 |  1200/  310 batches | lr 0.00 | ms/batch  8.15 | loss  0.47 | ppl     1.60\n",
            "| epoch  18 |  1300/  310 batches | lr 0.00 | ms/batch  8.26 | loss  0.36 | ppl     1.44\n",
            "| epoch  18 |  1400/  310 batches | lr 0.00 | ms/batch  8.34 | loss  0.39 | ppl     1.47\n",
            "| epoch  18 |  1500/  310 batches | lr 0.00 | ms/batch  8.16 | loss  0.44 | ppl     1.55\n",
            "| epoch  18 |  1600/  310 batches | lr 0.00 | ms/batch  8.24 | loss  0.41 | ppl     1.50\n",
            "| epoch  18 |  1700/  310 batches | lr 0.00 | ms/batch  8.59 | loss  0.39 | ppl     1.47\n",
            "| epoch  18 |  1800/  310 batches | lr 0.00 | ms/batch  8.26 | loss  0.44 | ppl     1.55\n",
            "| epoch  18 |  1900/  310 batches | lr 0.00 | ms/batch  8.21 | loss  0.52 | ppl     1.69\n",
            "| epoch  18 |  2000/  310 batches | lr 0.00 | ms/batch  8.21 | loss  0.41 | ppl     1.51\n",
            "| epoch  18 |  2100/  310 batches | lr 0.00 | ms/batch  8.20 | loss  0.44 | ppl     1.56\n",
            "| epoch  18 |  2200/  310 batches | lr 0.00 | ms/batch  8.59 | loss  0.44 | ppl     1.55\n",
            "| epoch  18 |  2300/  310 batches | lr 0.00 | ms/batch  8.36 | loss  0.41 | ppl     1.51\n",
            "| epoch  18 |  2400/  310 batches | lr 0.00 | ms/batch  8.23 | loss  0.41 | ppl     1.51\n",
            "| epoch  18 |  2500/  310 batches | lr 0.00 | ms/batch  7.85 | loss  0.45 | ppl     1.57\n",
            "| epoch  18 |  2600/  310 batches | lr 0.00 | ms/batch  8.11 | loss  0.42 | ppl     1.52\n",
            "| epoch  18 |  2700/  310 batches | lr 0.00 | ms/batch  8.17 | loss  0.42 | ppl     1.52\n",
            "| epoch  18 |  2800/  310 batches | lr 0.00 | ms/batch  8.30 | loss  0.40 | ppl     1.50\n",
            "| epoch  18 |  2900/  310 batches | lr 0.00 | ms/batch  8.04 | loss  0.44 | ppl     1.55\n",
            "| epoch  18 |  3000/  310 batches | lr 0.00 | ms/batch  7.96 | loss  0.44 | ppl     1.55\n",
            "| epoch  18 |  3100/  310 batches | lr 0.00 | ms/batch  7.97 | loss  0.45 | ppl     1.58\n",
            "| epoch  18 |  3200/  310 batches | lr 0.00 | ms/batch  8.38 | loss  0.43 | ppl     1.53\n",
            "| epoch  18 |  3300/  310 batches | lr 0.00 | ms/batch  8.65 | loss  0.53 | ppl     1.70\n",
            "| epoch  18 |  3400/  310 batches | lr 0.00 | ms/batch  8.42 | loss  0.50 | ppl     1.65\n",
            "| epoch  18 |  3500/  310 batches | lr 0.00 | ms/batch  8.30 | loss  0.53 | ppl     1.70\n",
            "| epoch  18 |  3600/  310 batches | lr 0.00 | ms/batch  8.19 | loss  0.46 | ppl     1.59\n",
            "| epoch  18 |  3700/  310 batches | lr 0.00 | ms/batch  7.97 | loss  0.50 | ppl     1.65\n",
            "| epoch  18 |  3800/  310 batches | lr 0.00 | ms/batch  8.23 | loss  0.50 | ppl     1.65\n",
            "| epoch  18 |  3900/  310 batches | lr 0.00 | ms/batch  8.00 | loss  0.45 | ppl     1.58\n",
            "| epoch  18 |  4000/  310 batches | lr 0.00 | ms/batch  8.40 | loss  0.36 | ppl     1.43\n",
            "| epoch  18 |  4100/  310 batches | lr 0.00 | ms/batch  8.03 | loss  0.33 | ppl     1.40\n",
            "| epoch  18 |  4200/  310 batches | lr 0.00 | ms/batch  8.33 | loss  0.40 | ppl     1.48\n",
            "| epoch  18 |  4300/  310 batches | lr 0.00 | ms/batch  7.99 | loss  0.42 | ppl     1.53\n",
            "| epoch  18 |  4400/  310 batches | lr 0.00 | ms/batch  7.99 | loss  0.49 | ppl     1.63\n",
            "| epoch  18 |  4500/  310 batches | lr 0.00 | ms/batch  7.97 | loss  0.48 | ppl     1.62\n",
            "| epoch  18 |  4600/  310 batches | lr 0.00 | ms/batch  8.40 | loss  0.49 | ppl     1.63\n",
            "| epoch  18 |  4700/  310 batches | lr 0.00 | ms/batch  8.08 | loss  0.43 | ppl     1.54\n",
            "| epoch  18 |  4800/  310 batches | lr 0.00 | ms/batch  8.30 | loss  0.41 | ppl     1.51\n",
            "| epoch  18 |  4900/  310 batches | lr 0.00 | ms/batch  8.22 | loss  0.41 | ppl     1.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time: 41.08s | valid loss  0.41 | valid ppl     1.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  19 |   100/  310 batches | lr 0.00 | ms/batch  8.23 | loss  0.44 | ppl     1.56\n",
            "| epoch  19 |   200/  310 batches | lr 0.00 | ms/batch  8.47 | loss  0.43 | ppl     1.53\n",
            "| epoch  19 |   300/  310 batches | lr 0.00 | ms/batch  8.24 | loss  0.40 | ppl     1.50\n",
            "| epoch  19 |   400/  310 batches | lr 0.00 | ms/batch  8.12 | loss  0.50 | ppl     1.65\n",
            "| epoch  19 |   500/  310 batches | lr 0.00 | ms/batch  8.36 | loss  0.42 | ppl     1.52\n",
            "| epoch  19 |   600/  310 batches | lr 0.00 | ms/batch  8.63 | loss  0.43 | ppl     1.54\n",
            "| epoch  19 |   700/  310 batches | lr 0.00 | ms/batch  8.67 | loss  0.45 | ppl     1.57\n",
            "| epoch  19 |   800/  310 batches | lr 0.00 | ms/batch  8.35 | loss  0.44 | ppl     1.55\n",
            "| epoch  19 |   900/  310 batches | lr 0.00 | ms/batch  8.48 | loss  0.38 | ppl     1.46\n",
            "| epoch  19 |  1000/  310 batches | lr 0.00 | ms/batch  8.43 | loss  0.41 | ppl     1.50\n",
            "| epoch  19 |  1100/  310 batches | lr 0.00 | ms/batch  8.38 | loss  0.39 | ppl     1.47\n",
            "| epoch  19 |  1200/  310 batches | lr 0.00 | ms/batch  8.17 | loss  0.47 | ppl     1.59\n",
            "| epoch  19 |  1300/  310 batches | lr 0.00 | ms/batch  8.41 | loss  0.36 | ppl     1.43\n",
            "| epoch  19 |  1400/  310 batches | lr 0.00 | ms/batch  8.52 | loss  0.38 | ppl     1.46\n",
            "| epoch  19 |  1500/  310 batches | lr 0.00 | ms/batch  8.23 | loss  0.44 | ppl     1.55\n",
            "| epoch  19 |  1600/  310 batches | lr 0.00 | ms/batch  8.67 | loss  0.41 | ppl     1.50\n",
            "| epoch  19 |  1700/  310 batches | lr 0.00 | ms/batch  8.05 | loss  0.38 | ppl     1.47\n",
            "| epoch  19 |  1800/  310 batches | lr 0.00 | ms/batch  8.20 | loss  0.44 | ppl     1.55\n",
            "| epoch  19 |  1900/  310 batches | lr 0.00 | ms/batch  8.10 | loss  0.52 | ppl     1.69\n",
            "| epoch  19 |  2000/  310 batches | lr 0.00 | ms/batch  8.26 | loss  0.41 | ppl     1.51\n",
            "| epoch  19 |  2100/  310 batches | lr 0.00 | ms/batch  8.27 | loss  0.44 | ppl     1.55\n",
            "| epoch  19 |  2200/  310 batches | lr 0.00 | ms/batch  8.07 | loss  0.44 | ppl     1.55\n",
            "| epoch  19 |  2300/  310 batches | lr 0.00 | ms/batch  8.20 | loss  0.41 | ppl     1.51\n",
            "| epoch  19 |  2400/  310 batches | lr 0.00 | ms/batch  8.82 | loss  0.41 | ppl     1.50\n",
            "| epoch  19 |  2500/  310 batches | lr 0.00 | ms/batch  8.61 | loss  0.45 | ppl     1.56\n",
            "| epoch  19 |  2600/  310 batches | lr 0.00 | ms/batch  8.29 | loss  0.42 | ppl     1.52\n",
            "| epoch  19 |  2700/  310 batches | lr 0.00 | ms/batch  8.29 | loss  0.41 | ppl     1.51\n",
            "| epoch  19 |  2800/  310 batches | lr 0.00 | ms/batch  8.25 | loss  0.40 | ppl     1.49\n",
            "| epoch  19 |  2900/  310 batches | lr 0.00 | ms/batch  8.43 | loss  0.44 | ppl     1.55\n",
            "| epoch  19 |  3000/  310 batches | lr 0.00 | ms/batch  8.25 | loss  0.44 | ppl     1.55\n",
            "| epoch  19 |  3100/  310 batches | lr 0.00 | ms/batch  8.25 | loss  0.45 | ppl     1.57\n",
            "| epoch  19 |  3200/  310 batches | lr 0.00 | ms/batch  8.13 | loss  0.43 | ppl     1.53\n",
            "| epoch  19 |  3300/  310 batches | lr 0.00 | ms/batch  8.40 | loss  0.52 | ppl     1.69\n",
            "| epoch  19 |  3400/  310 batches | lr 0.00 | ms/batch  8.90 | loss  0.50 | ppl     1.65\n",
            "| epoch  19 |  3500/  310 batches | lr 0.00 | ms/batch  8.32 | loss  0.53 | ppl     1.70\n",
            "| epoch  19 |  3600/  310 batches | lr 0.00 | ms/batch  8.50 | loss  0.46 | ppl     1.58\n",
            "| epoch  19 |  3700/  310 batches | lr 0.00 | ms/batch  8.10 | loss  0.50 | ppl     1.64\n",
            "| epoch  19 |  3800/  310 batches | lr 0.00 | ms/batch  8.34 | loss  0.50 | ppl     1.65\n",
            "| epoch  19 |  3900/  310 batches | lr 0.00 | ms/batch  8.85 | loss  0.46 | ppl     1.58\n",
            "| epoch  19 |  4000/  310 batches | lr 0.00 | ms/batch  8.41 | loss  0.36 | ppl     1.43\n",
            "| epoch  19 |  4100/  310 batches | lr 0.00 | ms/batch  8.24 | loss  0.34 | ppl     1.40\n",
            "| epoch  19 |  4200/  310 batches | lr 0.00 | ms/batch  8.36 | loss  0.39 | ppl     1.48\n",
            "| epoch  19 |  4300/  310 batches | lr 0.00 | ms/batch  8.39 | loss  0.42 | ppl     1.53\n",
            "| epoch  19 |  4400/  310 batches | lr 0.00 | ms/batch  8.36 | loss  0.49 | ppl     1.63\n",
            "| epoch  19 |  4500/  310 batches | lr 0.00 | ms/batch  8.48 | loss  0.48 | ppl     1.62\n",
            "| epoch  19 |  4600/  310 batches | lr 0.00 | ms/batch  8.34 | loss  0.48 | ppl     1.62\n",
            "| epoch  19 |  4700/  310 batches | lr 0.00 | ms/batch  8.29 | loss  0.43 | ppl     1.54\n",
            "| epoch  19 |  4800/  310 batches | lr 0.00 | ms/batch  8.78 | loss  0.41 | ppl     1.51\n",
            "| epoch  19 |  4900/  310 batches | lr 0.00 | ms/batch  8.27 | loss  0.41 | ppl     1.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time: 41.77s | valid loss  0.41 | valid ppl     1.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  20 |   100/  310 batches | lr 0.00 | ms/batch  8.51 | loss  0.44 | ppl     1.56\n",
            "| epoch  20 |   200/  310 batches | lr 0.00 | ms/batch  8.38 | loss  0.42 | ppl     1.53\n",
            "| epoch  20 |   300/  310 batches | lr 0.00 | ms/batch  8.26 | loss  0.40 | ppl     1.49\n",
            "| epoch  20 |   400/  310 batches | lr 0.00 | ms/batch  8.61 | loss  0.50 | ppl     1.65\n",
            "| epoch  20 |   500/  310 batches | lr 0.00 | ms/batch  8.40 | loss  0.42 | ppl     1.52\n",
            "| epoch  20 |   600/  310 batches | lr 0.00 | ms/batch  8.45 | loss  0.43 | ppl     1.53\n",
            "| epoch  20 |   700/  310 batches | lr 0.00 | ms/batch  8.37 | loss  0.44 | ppl     1.56\n",
            "| epoch  20 |   800/  310 batches | lr 0.00 | ms/batch  8.55 | loss  0.43 | ppl     1.54\n",
            "| epoch  20 |   900/  310 batches | lr 0.00 | ms/batch  8.26 | loss  0.38 | ppl     1.46\n",
            "| epoch  20 |  1000/  310 batches | lr 0.00 | ms/batch  8.29 | loss  0.41 | ppl     1.50\n",
            "| epoch  20 |  1100/  310 batches | lr 0.00 | ms/batch  8.33 | loss  0.39 | ppl     1.47\n",
            "| epoch  20 |  1200/  310 batches | lr 0.00 | ms/batch  8.41 | loss  0.46 | ppl     1.59\n",
            "| epoch  20 |  1300/  310 batches | lr 0.00 | ms/batch  8.44 | loss  0.36 | ppl     1.44\n",
            "| epoch  20 |  1400/  310 batches | lr 0.00 | ms/batch  8.33 | loss  0.38 | ppl     1.46\n",
            "| epoch  20 |  1500/  310 batches | lr 0.00 | ms/batch  8.25 | loss  0.44 | ppl     1.55\n",
            "| epoch  20 |  1600/  310 batches | lr 0.00 | ms/batch  8.49 | loss  0.40 | ppl     1.50\n",
            "| epoch  20 |  1700/  310 batches | lr 0.00 | ms/batch  8.40 | loss  0.38 | ppl     1.46\n",
            "| epoch  20 |  1800/  310 batches | lr 0.00 | ms/batch  8.62 | loss  0.44 | ppl     1.55\n",
            "| epoch  20 |  1900/  310 batches | lr 0.00 | ms/batch  8.26 | loss  0.52 | ppl     1.68\n",
            "| epoch  20 |  2000/  310 batches | lr 0.00 | ms/batch  8.50 | loss  0.41 | ppl     1.51\n",
            "| epoch  20 |  2100/  310 batches | lr 0.00 | ms/batch  8.84 | loss  0.44 | ppl     1.55\n",
            "| epoch  20 |  2200/  310 batches | lr 0.00 | ms/batch  8.48 | loss  0.43 | ppl     1.54\n",
            "| epoch  20 |  2300/  310 batches | lr 0.00 | ms/batch  8.25 | loss  0.41 | ppl     1.50\n",
            "| epoch  20 |  2400/  310 batches | lr 0.00 | ms/batch  8.24 | loss  0.41 | ppl     1.50\n",
            "| epoch  20 |  2500/  310 batches | lr 0.00 | ms/batch  8.31 | loss  0.45 | ppl     1.56\n",
            "| epoch  20 |  2600/  310 batches | lr 0.00 | ms/batch  8.57 | loss  0.41 | ppl     1.51\n",
            "| epoch  20 |  2700/  310 batches | lr 0.00 | ms/batch  8.54 | loss  0.42 | ppl     1.51\n",
            "| epoch  20 |  2800/  310 batches | lr 0.00 | ms/batch  8.65 | loss  0.40 | ppl     1.49\n",
            "| epoch  20 |  2900/  310 batches | lr 0.00 | ms/batch  8.18 | loss  0.44 | ppl     1.55\n",
            "| epoch  20 |  3000/  310 batches | lr 0.00 | ms/batch  8.12 | loss  0.44 | ppl     1.55\n",
            "| epoch  20 |  3100/  310 batches | lr 0.00 | ms/batch  8.13 | loss  0.45 | ppl     1.57\n",
            "| epoch  20 |  3200/  310 batches | lr 0.00 | ms/batch  8.33 | loss  0.43 | ppl     1.53\n",
            "| epoch  20 |  3300/  310 batches | lr 0.00 | ms/batch  8.25 | loss  0.52 | ppl     1.69\n",
            "| epoch  20 |  3400/  310 batches | lr 0.00 | ms/batch  8.11 | loss  0.50 | ppl     1.65\n",
            "| epoch  20 |  3500/  310 batches | lr 0.00 | ms/batch  9.30 | loss  0.53 | ppl     1.69\n",
            "| epoch  20 |  3600/  310 batches | lr 0.00 | ms/batch  8.87 | loss  0.46 | ppl     1.58\n",
            "| epoch  20 |  3700/  310 batches | lr 0.00 | ms/batch  8.37 | loss  0.50 | ppl     1.64\n",
            "| epoch  20 |  3800/  310 batches | lr 0.00 | ms/batch  8.20 | loss  0.50 | ppl     1.64\n",
            "| epoch  20 |  3900/  310 batches | lr 0.00 | ms/batch  8.34 | loss  0.45 | ppl     1.57\n",
            "| epoch  20 |  4000/  310 batches | lr 0.00 | ms/batch  8.36 | loss  0.36 | ppl     1.43\n",
            "| epoch  20 |  4100/  310 batches | lr 0.00 | ms/batch  8.44 | loss  0.33 | ppl     1.39\n",
            "| epoch  20 |  4200/  310 batches | lr 0.00 | ms/batch  8.45 | loss  0.39 | ppl     1.48\n",
            "| epoch  20 |  4300/  310 batches | lr 0.00 | ms/batch  8.52 | loss  0.42 | ppl     1.52\n",
            "| epoch  20 |  4400/  310 batches | lr 0.00 | ms/batch  8.67 | loss  0.49 | ppl     1.63\n",
            "| epoch  20 |  4500/  310 batches | lr 0.00 | ms/batch  8.39 | loss  0.48 | ppl     1.61\n",
            "| epoch  20 |  4600/  310 batches | lr 0.00 | ms/batch  8.23 | loss  0.48 | ppl     1.62\n",
            "| epoch  20 |  4700/  310 batches | lr 0.00 | ms/batch  8.16 | loss  0.43 | ppl     1.54\n",
            "| epoch  20 |  4800/  310 batches | lr 0.00 | ms/batch  8.20 | loss  0.41 | ppl     1.50\n",
            "| epoch  20 |  4900/  310 batches | lr 0.00 | ms/batch  8.71 | loss  0.41 | ppl     1.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time: 42.02s | valid loss  0.41 | valid ppl     1.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  21 |   100/  310 batches | lr 0.00 | ms/batch  8.22 | loss  0.44 | ppl     1.55\n",
            "| epoch  21 |   200/  310 batches | lr 0.00 | ms/batch  8.33 | loss  0.42 | ppl     1.52\n",
            "| epoch  21 |   300/  310 batches | lr 0.00 | ms/batch  8.28 | loss  0.40 | ppl     1.49\n",
            "| epoch  21 |   400/  310 batches | lr 0.00 | ms/batch  8.63 | loss  0.50 | ppl     1.64\n",
            "| epoch  21 |   500/  310 batches | lr 0.00 | ms/batch  8.44 | loss  0.42 | ppl     1.52\n",
            "| epoch  21 |   600/  310 batches | lr 0.00 | ms/batch  8.41 | loss  0.43 | ppl     1.53\n",
            "| epoch  21 |   700/  310 batches | lr 0.00 | ms/batch  8.74 | loss  0.44 | ppl     1.56\n",
            "| epoch  21 |   800/  310 batches | lr 0.00 | ms/batch  8.46 | loss  0.43 | ppl     1.54\n",
            "| epoch  21 |   900/  310 batches | lr 0.00 | ms/batch  8.39 | loss  0.38 | ppl     1.46\n",
            "| epoch  21 |  1000/  310 batches | lr 0.00 | ms/batch  8.37 | loss  0.41 | ppl     1.50\n",
            "| epoch  21 |  1100/  310 batches | lr 0.00 | ms/batch  8.27 | loss  0.38 | ppl     1.46\n",
            "| epoch  21 |  1200/  310 batches | lr 0.00 | ms/batch  8.53 | loss  0.46 | ppl     1.58\n",
            "| epoch  21 |  1300/  310 batches | lr 0.00 | ms/batch  8.42 | loss  0.36 | ppl     1.43\n",
            "| epoch  21 |  1400/  310 batches | lr 0.00 | ms/batch  8.56 | loss  0.38 | ppl     1.46\n",
            "| epoch  21 |  1500/  310 batches | lr 0.00 | ms/batch  8.41 | loss  0.43 | ppl     1.54\n",
            "| epoch  21 |  1600/  310 batches | lr 0.00 | ms/batch  8.43 | loss  0.40 | ppl     1.49\n",
            "| epoch  21 |  1700/  310 batches | lr 0.00 | ms/batch  8.15 | loss  0.38 | ppl     1.46\n",
            "| epoch  21 |  1800/  310 batches | lr 0.00 | ms/batch  8.31 | loss  0.43 | ppl     1.54\n",
            "| epoch  21 |  1900/  310 batches | lr 0.00 | ms/batch  8.50 | loss  0.52 | ppl     1.68\n",
            "| epoch  21 |  2000/  310 batches | lr 0.00 | ms/batch  8.94 | loss  0.41 | ppl     1.50\n",
            "| epoch  21 |  2100/  310 batches | lr 0.00 | ms/batch  8.37 | loss  0.44 | ppl     1.55\n",
            "| epoch  21 |  2200/  310 batches | lr 0.00 | ms/batch  8.63 | loss  0.43 | ppl     1.54\n",
            "| epoch  21 |  2300/  310 batches | lr 0.00 | ms/batch  8.37 | loss  0.41 | ppl     1.50\n",
            "| epoch  21 |  2400/  310 batches | lr 0.00 | ms/batch  8.53 | loss  0.40 | ppl     1.50\n",
            "| epoch  21 |  2500/  310 batches | lr 0.00 | ms/batch  8.91 | loss  0.44 | ppl     1.55\n",
            "| epoch  21 |  2600/  310 batches | lr 0.00 | ms/batch  8.88 | loss  0.41 | ppl     1.51\n",
            "| epoch  21 |  2700/  310 batches | lr 0.00 | ms/batch  8.40 | loss  0.41 | ppl     1.51\n",
            "| epoch  21 |  2800/  310 batches | lr 0.00 | ms/batch  7.97 | loss  0.40 | ppl     1.49\n",
            "| epoch  21 |  2900/  310 batches | lr 0.00 | ms/batch  8.16 | loss  0.43 | ppl     1.54\n",
            "| epoch  21 |  3000/  310 batches | lr 0.00 | ms/batch  8.50 | loss  0.43 | ppl     1.54\n",
            "| epoch  21 |  3100/  310 batches | lr 0.00 | ms/batch  8.37 | loss  0.45 | ppl     1.56\n",
            "| epoch  21 |  3200/  310 batches | lr 0.00 | ms/batch  8.26 | loss  0.42 | ppl     1.52\n",
            "| epoch  21 |  3300/  310 batches | lr 0.00 | ms/batch  8.38 | loss  0.52 | ppl     1.68\n",
            "| epoch  21 |  3400/  310 batches | lr 0.00 | ms/batch  8.45 | loss  0.50 | ppl     1.65\n",
            "| epoch  21 |  3500/  310 batches | lr 0.00 | ms/batch  8.56 | loss  0.53 | ppl     1.69\n",
            "| epoch  21 |  3600/  310 batches | lr 0.00 | ms/batch  8.64 | loss  0.46 | ppl     1.58\n",
            "| epoch  21 |  3700/  310 batches | lr 0.00 | ms/batch  8.23 | loss  0.49 | ppl     1.64\n",
            "| epoch  21 |  3800/  310 batches | lr 0.00 | ms/batch  8.33 | loss  0.50 | ppl     1.64\n",
            "| epoch  21 |  3900/  310 batches | lr 0.00 | ms/batch  8.20 | loss  0.45 | ppl     1.57\n",
            "| epoch  21 |  4000/  310 batches | lr 0.00 | ms/batch  8.12 | loss  0.36 | ppl     1.43\n",
            "| epoch  21 |  4100/  310 batches | lr 0.00 | ms/batch  8.65 | loss  0.33 | ppl     1.39\n",
            "| epoch  21 |  4200/  310 batches | lr 0.00 | ms/batch  8.45 | loss  0.39 | ppl     1.48\n",
            "| epoch  21 |  4300/  310 batches | lr 0.00 | ms/batch  8.36 | loss  0.42 | ppl     1.52\n",
            "| epoch  21 |  4400/  310 batches | lr 0.00 | ms/batch  8.15 | loss  0.48 | ppl     1.62\n",
            "| epoch  21 |  4500/  310 batches | lr 0.00 | ms/batch  8.25 | loss  0.48 | ppl     1.61\n",
            "| epoch  21 |  4600/  310 batches | lr 0.00 | ms/batch  8.67 | loss  0.48 | ppl     1.62\n",
            "| epoch  21 |  4700/  310 batches | lr 0.00 | ms/batch  8.20 | loss  0.43 | ppl     1.53\n",
            "| epoch  21 |  4800/  310 batches | lr 0.00 | ms/batch  8.37 | loss  0.40 | ppl     1.50\n",
            "| epoch  21 |  4900/  310 batches | lr 0.00 | ms/batch  8.44 | loss  0.41 | ppl     1.50\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time: 41.97s | valid loss  0.41 | valid ppl     1.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  22 |   100/  310 batches | lr 0.00 | ms/batch  8.46 | loss  0.44 | ppl     1.55\n",
            "| epoch  22 |   200/  310 batches | lr 0.00 | ms/batch  8.21 | loss  0.42 | ppl     1.52\n",
            "| epoch  22 |   300/  310 batches | lr 0.00 | ms/batch  9.05 | loss  0.40 | ppl     1.49\n",
            "| epoch  22 |   400/  310 batches | lr 0.00 | ms/batch  8.60 | loss  0.50 | ppl     1.64\n",
            "| epoch  22 |   500/  310 batches | lr 0.00 | ms/batch  8.33 | loss  0.42 | ppl     1.52\n",
            "| epoch  22 |   600/  310 batches | lr 0.00 | ms/batch  8.45 | loss  0.43 | ppl     1.53\n",
            "| epoch  22 |   700/  310 batches | lr 0.00 | ms/batch  8.33 | loss  0.44 | ppl     1.55\n",
            "| epoch  22 |   800/  310 batches | lr 0.00 | ms/batch  8.11 | loss  0.43 | ppl     1.54\n",
            "| epoch  22 |   900/  310 batches | lr 0.00 | ms/batch  8.50 | loss  0.37 | ppl     1.45\n",
            "| epoch  22 |  1000/  310 batches | lr 0.00 | ms/batch  8.65 | loss  0.40 | ppl     1.50\n",
            "| epoch  22 |  1100/  310 batches | lr 0.00 | ms/batch  8.34 | loss  0.38 | ppl     1.46\n",
            "| epoch  22 |  1200/  310 batches | lr 0.00 | ms/batch  8.21 | loss  0.46 | ppl     1.58\n",
            "| epoch  22 |  1300/  310 batches | lr 0.00 | ms/batch  8.27 | loss  0.36 | ppl     1.43\n",
            "| epoch  22 |  1400/  310 batches | lr 0.00 | ms/batch  8.34 | loss  0.38 | ppl     1.46\n",
            "| epoch  22 |  1500/  310 batches | lr 0.00 | ms/batch  8.36 | loss  0.43 | ppl     1.54\n",
            "| epoch  22 |  1600/  310 batches | lr 0.00 | ms/batch  8.18 | loss  0.40 | ppl     1.49\n",
            "| epoch  22 |  1700/  310 batches | lr 0.00 | ms/batch  7.99 | loss  0.37 | ppl     1.45\n",
            "| epoch  22 |  1800/  310 batches | lr 0.00 | ms/batch  8.56 | loss  0.43 | ppl     1.54\n",
            "| epoch  22 |  1900/  310 batches | lr 0.00 | ms/batch  8.37 | loss  0.52 | ppl     1.68\n",
            "| epoch  22 |  2000/  310 batches | lr 0.00 | ms/batch  8.53 | loss  0.40 | ppl     1.50\n",
            "| epoch  22 |  2100/  310 batches | lr 0.00 | ms/batch  8.30 | loss  0.43 | ppl     1.54\n",
            "| epoch  22 |  2200/  310 batches | lr 0.00 | ms/batch  8.01 | loss  0.43 | ppl     1.54\n",
            "| epoch  22 |  2300/  310 batches | lr 0.00 | ms/batch  8.37 | loss  0.40 | ppl     1.50\n",
            "| epoch  22 |  2400/  310 batches | lr 0.00 | ms/batch  8.19 | loss  0.40 | ppl     1.50\n",
            "| epoch  22 |  2500/  310 batches | lr 0.00 | ms/batch  8.18 | loss  0.44 | ppl     1.55\n",
            "| epoch  22 |  2600/  310 batches | lr 0.00 | ms/batch  8.18 | loss  0.41 | ppl     1.51\n",
            "| epoch  22 |  2700/  310 batches | lr 0.00 | ms/batch  8.39 | loss  0.41 | ppl     1.51\n",
            "| epoch  22 |  2800/  310 batches | lr 0.00 | ms/batch  8.48 | loss  0.39 | ppl     1.48\n",
            "| epoch  22 |  2900/  310 batches | lr 0.00 | ms/batch  8.33 | loss  0.43 | ppl     1.54\n",
            "| epoch  22 |  3000/  310 batches | lr 0.00 | ms/batch  8.02 | loss  0.43 | ppl     1.54\n",
            "| epoch  22 |  3100/  310 batches | lr 0.00 | ms/batch  8.03 | loss  0.45 | ppl     1.57\n",
            "| epoch  22 |  3200/  310 batches | lr 0.00 | ms/batch  8.15 | loss  0.42 | ppl     1.52\n",
            "| epoch  22 |  3300/  310 batches | lr 0.00 | ms/batch  8.19 | loss  0.52 | ppl     1.68\n",
            "| epoch  22 |  3400/  310 batches | lr 0.00 | ms/batch  8.33 | loss  0.50 | ppl     1.65\n",
            "| epoch  22 |  3500/  310 batches | lr 0.00 | ms/batch  8.14 | loss  0.52 | ppl     1.69\n",
            "| epoch  22 |  3600/  310 batches | lr 0.00 | ms/batch  8.38 | loss  0.46 | ppl     1.58\n",
            "| epoch  22 |  3700/  310 batches | lr 0.00 | ms/batch  8.21 | loss  0.49 | ppl     1.64\n",
            "| epoch  22 |  3800/  310 batches | lr 0.00 | ms/batch  8.04 | loss  0.49 | ppl     1.64\n",
            "| epoch  22 |  3900/  310 batches | lr 0.00 | ms/batch  8.21 | loss  0.45 | ppl     1.57\n",
            "| epoch  22 |  4000/  310 batches | lr 0.00 | ms/batch  8.46 | loss  0.35 | ppl     1.42\n",
            "| epoch  22 |  4100/  310 batches | lr 0.00 | ms/batch  8.58 | loss  0.33 | ppl     1.39\n",
            "| epoch  22 |  4200/  310 batches | lr 0.00 | ms/batch  8.21 | loss  0.39 | ppl     1.48\n",
            "| epoch  22 |  4300/  310 batches | lr 0.00 | ms/batch  8.49 | loss  0.42 | ppl     1.52\n",
            "| epoch  22 |  4400/  310 batches | lr 0.00 | ms/batch  8.71 | loss  0.48 | ppl     1.62\n",
            "| epoch  22 |  4500/  310 batches | lr 0.00 | ms/batch  8.54 | loss  0.48 | ppl     1.61\n",
            "| epoch  22 |  4600/  310 batches | lr 0.00 | ms/batch  8.87 | loss  0.48 | ppl     1.62\n",
            "| epoch  22 |  4700/  310 batches | lr 0.00 | ms/batch  8.33 | loss  0.42 | ppl     1.53\n",
            "| epoch  22 |  4800/  310 batches | lr 0.00 | ms/batch  8.45 | loss  0.40 | ppl     1.50\n",
            "| epoch  22 |  4900/  310 batches | lr 0.00 | ms/batch  8.68 | loss  0.40 | ppl     1.50\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time: 41.68s | valid loss  0.41 | valid ppl     1.50\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  23 |   100/  310 batches | lr 0.00 | ms/batch  8.49 | loss  0.44 | ppl     1.55\n",
            "| epoch  23 |   200/  310 batches | lr 0.00 | ms/batch  8.22 | loss  0.42 | ppl     1.52\n",
            "| epoch  23 |   300/  310 batches | lr 0.00 | ms/batch  8.40 | loss  0.40 | ppl     1.49\n",
            "| epoch  23 |   400/  310 batches | lr 0.00 | ms/batch  9.32 | loss  0.50 | ppl     1.64\n",
            "| epoch  23 |   500/  310 batches | lr 0.00 | ms/batch  8.76 | loss  0.42 | ppl     1.52\n",
            "| epoch  23 |   600/  310 batches | lr 0.00 | ms/batch  8.13 | loss  0.42 | ppl     1.53\n",
            "| epoch  23 |   700/  310 batches | lr 0.00 | ms/batch  8.28 | loss  0.44 | ppl     1.56\n",
            "| epoch  23 |   800/  310 batches | lr 0.00 | ms/batch  8.44 | loss  0.43 | ppl     1.54\n",
            "| epoch  23 |   900/  310 batches | lr 0.00 | ms/batch  8.51 | loss  0.37 | ppl     1.45\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-c9401ec9b7b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mepoch_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mval_ppl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-37-37908b35d960>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mclip_coef_clamped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip_coef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip_coef_clamped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1102\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "def vect2word(vect):\n",
        "  vect = vect[0].detach().to(\"cpu\").numpy()\n",
        "  word = []\n",
        "  for p in vect:\n",
        "    if p > 0:\n",
        "      word.append(index_to_phon[p])\n",
        "  return \" \".join(word)\n",
        "\n",
        "for i in range(25):\n",
        "  r1 , r2 = random.randint(0,len(train_data) - 1), random.randint(0,len(train_data[0])-1)\n",
        "  ex = train_data[r1][r2].unsqueeze(0)\n",
        "  mask = generate_square_subsequent_mask(1).to(device)\n",
        "  res = best_model(ex, mask)\n",
        "  resmax = torch.max(res,dim=2)\n",
        "  # print(resmax.indices)\n",
        "\n",
        "\n",
        "  print(\"example\", vect2word(ex)[4:-4])\n",
        "  print(\"result\", vect2word(resmax.indices)[4:-4])\n",
        "  print()\n",
        "\n",
        "\n",
        "wrd = \"BEG B AE1 G S END\"\n",
        "\n",
        "ex = torch.tensor([[phonemes[p] for p in wrd.split(\" \")]]).to(device)\n",
        "mask = generate_square_subsequent_mask(1).to(device)\n",
        "res = best_model(ex, mask)\n",
        "resmax = torch.max(res,dim=2)\n",
        "print(\"example\", vect2word(ex)[4:-4])\n",
        "print(\"result\", vect2word(resmax.indices)[4:-4])\n",
        "print(\"vals\", resmax.values[0][1:-1])\n",
        "\n",
        "wrd = \"BEG B AE1 G Z END\"\n",
        "\n",
        "ex = torch.tensor([[phonemes[p] for p in wrd.split(\" \")]]).to(device)\n",
        "mask = generate_square_subsequent_mask(1).to(device)\n",
        "res = best_model(ex, mask)\n",
        "resmax = torch.max(res,dim=2)\n",
        "print(\"example\", vect2word(ex)[4:-4])\n",
        "print(\"result\", vect2word(resmax.indices)[4:-4])\n",
        "print(\"vals\", resmax.values[0][1:-1])\n",
        "\n",
        "# bla = torch.tensor([[phonemes[\"D\"], phonemes[\"T\"]]]).to(device)\n",
        "# encoding = best_model.encoder(bla)[0]\n",
        "# print(encoding)\n",
        "# print(encoding[0] - encoding[1])\n",
        "\n",
        "# bla = torch.tensor([[phonemes[\"D\"], phonemes[\"N\"]]]).to(device)\n",
        "# encoding = best_model.encoder(bla)[0]\n",
        "# print(encoding)\n",
        "# print(encoding[0] - encoding[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7C_rsMdhf5y",
        "outputId": "8e70a653-64a6-460f-95c9-51a4d437d97e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "example EH1 R G AE2 S\n",
            "result EH1 R G AE2 S\n",
            "\n",
            "example B R IH1 T N IY2\n",
            "result B R IH1 T N IH1\n",
            "\n",
            "example M AA0 R S EH1 S AH0\n",
            "result M AA0 R S EH1 S AH0\n",
            "\n",
            "example EH1 M B ER0\n",
            "result EH1 M B ER0\n",
            "\n",
            "example G AE2 L AH0 L IY1 OW0 Z\n",
            "result G AE2 L AH0 L IY1 OW0 Z\n",
            "\n",
            "example B R AO1 D HH ER0 S T\n",
            "result B R AE1 D HH ER0 S T\n",
            "\n",
            "example F AO1 R T N AH0\n",
            "result F AE1 R T N AH0\n",
            "\n",
            "example G AO1 N T\n",
            "result G AE1 N T\n",
            "\n",
            "example B EH1 R IH0 SH\n",
            "result B EH1 R IH0 SH\n",
            "\n",
            "example B AH1 F AH0 T\n",
            "result B AH1 F AH0 T\n",
            "\n",
            "example P AE1 K\n",
            "result P AE1 K\n",
            "\n",
            "example HH AE1 R AH0\n",
            "result HH AE1 R AH0\n",
            "\n",
            "example L IY1 CH\n",
            "result L IY1 CH\n",
            "\n",
            "example M AE1 S AH0 K IH0 S T\n",
            "result M AE1 S AH0 K IH0 S T\n",
            "\n",
            "example IH2 M P EY1 L D\n",
            "result END M P EY1 L D\n",
            "\n",
            "example P IH1 T AH0 D\n",
            "result P IH1 T AH0 D\n",
            "\n",
            "example K AH1 B Z\n",
            "result K AH1 B Z\n",
            "\n",
            "example B AA0 S T EY1 D OW0\n",
            "result B AA0 S T EY1 D OW0\n",
            "\n",
            "example K R IH1 S AH0 N IH0 NG\n",
            "result K R IH1 S AH0 N IH0 NG\n",
            "\n",
            "example K AA1 M P OW0 S T\n",
            "result K AA1 M P OW0 S T\n",
            "\n",
            "example S IH1 N AH0 M AE0 K S\n",
            "result S IH1 N AH0 M EY1 K S\n",
            "\n",
            "example B OY1\n",
            "result B IH1\n",
            "\n",
            "example M AH1 L D R AW0\n",
            "result M AH1 L D R OW1\n",
            "\n",
            "example M AH0 G AE1 L IY0 Z\n",
            "result M AH0 G AE1 L IY0 Z\n",
            "\n",
            "example P AY1 R AH0 T S\n",
            "result P IY1 R AH0 T S\n",
            "\n",
            "example B AE1 G S\n",
            "result B AE1 G S\n",
            "vals tensor([10.0280,  9.4347,  8.7286, 10.1196], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "example B AE1 G Z\n",
            "result B AE1 G Z\n",
            "vals tensor([10.0280,  9.4347,  8.7286, 10.0589], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m = nn.Softmax(dim=2)\n",
        "r = best_model(ex, mask)\n",
        "print(r.shape)\n",
        "sft = m(r)\n",
        "print(torch.sum(r, dim = 2))\n",
        "print(torch.sum(sft, dim = 2))\n",
        "print(sft)\n",
        "\n",
        "# print(m(best_model(ex, mask)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXvOODeQJlQx",
        "outputId": "84967437-5dcc-4937-e864-8ab91cbdabf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 4, 70])\n",
            "tensor([[-26.3453, -27.0656,  17.0586, -91.2527]], device='cuda:0',\n",
            "       grad_fn=<SumBackward1>)\n",
            "tensor([[1.0000, 1.0000, 1.0000, 1.0000]], device='cuda:0',\n",
            "       grad_fn=<SumBackward1>)\n",
            "tensor([[[9.5487e-07, 5.7577e-05, 3.0597e-08, 7.8769e-11, 3.6094e-07,\n",
            "          7.4658e-06, 3.7032e-08, 7.5257e-13, 1.4633e-10, 1.0678e-07,\n",
            "          1.9013e-10, 1.1392e-06, 1.9536e-11, 5.6422e-09, 1.3081e-13,\n",
            "          4.3037e-09, 4.3951e-07, 1.2884e-08, 9.6486e-05, 1.3890e-09,\n",
            "          6.2488e-07, 4.6038e-05, 9.7254e-09, 2.6380e-05, 9.7611e-07,\n",
            "          9.9968e-01, 3.3579e-05, 1.2219e-10, 2.2879e-11, 5.1743e-08,\n",
            "          7.9277e-11, 7.4144e-07, 1.6101e-11, 2.8078e-10, 2.5263e-10,\n",
            "          4.9425e-07, 1.1511e-09, 3.3990e-08, 1.2757e-05, 5.2381e-08,\n",
            "          3.5845e-09, 7.2815e-10, 1.1097e-11, 1.2549e-06, 4.6851e-07,\n",
            "          2.6210e-07, 1.7313e-06, 8.3643e-07, 2.9552e-08, 7.3294e-07,\n",
            "          7.2210e-08, 2.6002e-06, 2.1903e-08, 3.0060e-07, 4.2197e-07,\n",
            "          3.1859e-07, 4.0386e-06, 1.0050e-06, 1.3813e-05, 8.0405e-09,\n",
            "          5.1662e-08, 2.7039e-10, 1.3248e-07, 2.8169e-08, 2.3409e-07,\n",
            "          7.3307e-08, 4.3742e-07, 9.4255e-08, 1.1573e-07, 2.0488e-07],\n",
            "         [5.3605e-07, 2.4824e-12, 3.4616e-06, 6.2653e-04, 3.0097e-05,\n",
            "          2.5720e-09, 6.2233e-08, 4.0839e-03, 7.5420e-10, 1.0937e-10,\n",
            "          2.3087e-03, 1.2902e-08, 3.5072e-03, 9.8623e-08, 9.7361e-01,\n",
            "          4.0171e-08, 5.9243e-09, 4.5417e-06, 7.3273e-14, 1.5495e-05,\n",
            "          7.1599e-11, 1.2236e-07, 2.1639e-03, 9.5709e-11, 5.9454e-09,\n",
            "          3.1784e-12, 1.9091e-09, 3.5893e-03, 5.3144e-06, 6.3307e-05,\n",
            "          1.0525e-05, 1.0641e-05, 6.9830e-07, 9.1638e-03, 1.0814e-04,\n",
            "          1.1410e-08, 2.2900e-06, 2.0279e-09, 2.3476e-10, 8.3309e-10,\n",
            "          2.6030e-06, 3.6875e-09, 5.6963e-04, 7.8052e-11, 2.2492e-09,\n",
            "          2.7992e-09, 2.8928e-09, 1.8959e-10, 2.9983e-08, 3.1519e-10,\n",
            "          9.5900e-10, 3.3113e-09, 1.7120e-09, 1.8364e-08, 5.6462e-10,\n",
            "          6.1821e-09, 7.5053e-11, 1.2282e-09, 8.2112e-12, 5.0631e-11,\n",
            "          6.9964e-10, 1.1460e-04, 9.9600e-10, 1.0438e-09, 7.1621e-09,\n",
            "          8.4834e-10, 9.4156e-09, 4.7637e-09, 5.9659e-09, 9.7555e-10],\n",
            "         [3.1865e-10, 1.1034e-06, 7.1973e-05, 2.0800e-06, 3.1881e-08,\n",
            "          2.0588e-06, 2.2595e-07, 1.7719e-05, 4.7502e-05, 1.2809e-04,\n",
            "          4.8926e-08, 6.3775e-08, 1.0013e-06, 6.8610e-07, 2.8684e-06,\n",
            "          1.0033e-08, 7.8713e-08, 1.6663e-05, 9.6072e-07, 2.8887e-07,\n",
            "          9.1735e-09, 1.0981e-06, 3.0294e-09, 5.8313e-09, 3.5353e-09,\n",
            "          1.9736e-11, 9.1443e-10, 5.8061e-08, 9.4355e-05, 9.2129e-08,\n",
            "          2.6224e-03, 9.3304e-12, 9.9597e-01, 5.9032e-08, 2.3698e-07,\n",
            "          1.4895e-09, 3.9728e-06, 2.5872e-10, 2.6088e-12, 3.5814e-09,\n",
            "          3.5326e-07, 1.2014e-04, 4.5474e-06, 2.3778e-10, 7.6112e-08,\n",
            "          4.0205e-09, 5.0518e-08, 2.6587e-09, 5.9726e-04, 4.0054e-09,\n",
            "          1.2858e-06, 5.4949e-08, 2.4711e-07, 1.3978e-09, 7.4268e-10,\n",
            "          3.4414e-08, 3.7687e-09, 1.4864e-04, 4.3111e-09, 1.4213e-04,\n",
            "          1.4044e-07, 8.8324e-08, 2.6435e-08, 8.9402e-07, 2.6662e-08,\n",
            "          9.2055e-08, 1.5140e-07, 5.8812e-08, 2.9808e-07, 1.3130e-07],\n",
            "         [3.9618e-06, 6.4833e-07, 3.2015e-04, 1.4154e-07, 9.9776e-01,\n",
            "          3.7552e-11, 1.2573e-03, 1.1857e-07, 2.1207e-09, 4.8798e-08,\n",
            "          9.0134e-08, 1.5816e-10, 1.2387e-07, 3.1538e-12, 6.9719e-06,\n",
            "          3.1633e-12, 1.1444e-06, 7.9769e-06, 3.9232e-12, 2.3226e-09,\n",
            "          2.9118e-11, 1.3399e-05, 9.0750e-05, 4.7891e-05, 2.5968e-07,\n",
            "          1.1565e-06, 1.4835e-08, 7.3239e-07, 2.4093e-08, 2.9458e-07,\n",
            "          4.8040e-04, 1.9729e-06, 2.2105e-08, 6.5299e-06, 1.4431e-08,\n",
            "          7.1875e-07, 6.8614e-10, 2.1605e-11, 9.1664e-08, 2.6127e-10,\n",
            "          5.1832e-09, 1.7185e-10, 5.5383e-09, 1.0022e-09, 8.8153e-08,\n",
            "          4.2583e-07, 1.6243e-07, 3.9407e-10, 1.2542e-11, 1.9112e-08,\n",
            "          1.2810e-10, 6.4395e-08, 7.9627e-10, 4.0081e-07, 1.2142e-09,\n",
            "          9.7698e-08, 8.9996e-12, 8.9792e-08, 2.8103e-11, 4.7529e-13,\n",
            "          2.6867e-10, 5.0594e-09, 2.8895e-09, 3.3012e-11, 3.1579e-08,\n",
            "          1.4022e-10, 1.4036e-07, 1.8212e-09, 1.0274e-09, 1.0536e-09]]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    }
  ]
}